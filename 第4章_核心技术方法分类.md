# 第 4 章 核心技术方法分类

在第 3 章中，我们系统地定义了在线多模态理解（OMU）的研究框架，明确了其核心约束条件和本质特征。本章将深入分析实现 OMU 的核心技术方法，从时间建模、模态融合、未来预测到高效推理四个维度进行系统分类，并对比不同技术路线的优劣。

## 4.1 时间建模方法

时间建模是 OMU 的基础，负责捕捉多模态序列中的时序依赖关系。与离线方法可使用双向建模不同，OMU 必须采用满足因果性约束的单向时间建模机制。

**关于复杂度与延迟口径**：除非特别说明，本章"单步/每帧复杂度"按**推理时序步**计。对循环与注意力模型，严格单步计算复杂度可写为：
- RNN/LSTM/GRU：O(d²ₕ + dₕdₓ)；若以"门数"近似，LSTM ≈ 4×RNN，GRU ≈ 3×RNN。
- Transformer（无限缓存）：单步 O(t·d) / 内存 O(t·d)。
- Transformer（固定窗口 L）：单步 **O(L·d)** / 内存 **O(L·d)**（与 t 无关）。

文中表格的"单步延迟"默认测试条件为 **batch=1、224×224、FP16（若适用）、V100**；不同骨干/分辨率/硬件将显著改变绝对数值。

### 4.1.1 循环神经网络（RNN）及其变体

**基本原理**：RNN 通过递归更新隐状态来建模序列，天然满足因果性约束。在时刻 t，隐状态 hₜ 仅依赖当前输入 xₜ 和前一时刻隐状态 hₜ₋₁：

```
hₜ = σ(Wₕ·hₜ₋₁ + Wₓ·xₜ + b)
```

**主要变体**：

**（1）标准 RNN**：
- **优势**：结构简单，参数量少
- **局限**：梯度消失/爆炸问题严重，难以捕捉长程依赖（有效记忆长度通常 <10 步）[1]
- **适用场景**：短序列任务（<20 帧），资源极度受限环境

**（2）长短期记忆网络（LSTM）** [2]：
- **核心机制**：通过输入门、遗忘门、输出门控制信息流动
- **隐状态更新**：
  ```
  fₜ = σ(Wf·[hₜ₋₁, xₜ] + bf)  # 遗忘门
  iₜ = σ(Wi·[hₜ₋₁, xₜ] + bi)  # 输入门
  c̃ₜ = tanh(Wc·[hₜ₋₁, xₜ] + bc)  # 候选记忆
  cₜ = fₜ ⊙ cₜ₋₁ + iₜ ⊙ c̃ₜ  # 记忆单元更新
  oₜ = σ(Wo·[hₜ₋₁, xₜ] + bo)  # 输出门
  hₜ = oₜ ⊙ tanh(cₜ)  # 隐状态输出
  ```
- **优势**：缓解梯度消失，有效记忆长度可达 50-100 步
- **参数量**：约为标准 RNN 的 **4 倍**（4 个门控单元）
- **计算复杂度**：单步 O(4·(d²ₕ + dₕdₓ))；工程口径常近似为 **O(dₕ)**（若把矩阵乘并行和常数折算入核实现）

**（3）门控循环单元（GRU）** [3]：
- **简化设计**：合并遗忘门和输入门为更新门，取消记忆单元
- **隐状态更新**：
  ```
  zₜ = σ(Wz·[hₜ₋₁, xₜ])  # 更新门
  rₜ = σ(Wr·[hₜ₋₁, xₜ])  # 重置门
  h̃ₜ = tanh(W·[rₜ ⊙ hₜ₋₁, xₜ])  # 候选隐状态
  hₜ = (1-zₜ) ⊙ hₜ₋₁ + zₜ ⊙ h̃ₜ  # 插值更新
  ```
- **参数量**：约为 LSTM 的 **75%**（3 个门 vs 4 个门）
- **计算复杂度**：单步 O(3·(d²ₕ + dₕdₓ))；工程口径常近似为 **O(dₕ)**（若把矩阵乘并行和常数折算入核实现）
- **性能对比**：在多数任务上与 LSTM 相当，但训练速度快 20-30% [4]

**RNN 变体性能对比**（AVE 数据集，在线模式，隐状态维度 d=512）：

| 模型 | Segment F1 (%) | 单步延迟 (ms) | 参数量 (M) | 有效记忆长度 | 训练稳定性 |
|------|---------------|--------------|-----------|------------|----------|
| 标准 RNN | 64-66 | 8 | 2.1 | <10 步 | 差 |
| LSTM | 68-70 | 18 | 8.5 | 50-100 步 | 良好 |
| GRU | 67-69 | 14 | 6.4 | 40-80 步 | 良好 |

*注：单步延迟为**典型范围/示例**（batch=1、224×224、FP32、V100），不同实现与硬件会有显著差异；有效记忆长度指梯度幅值降至初始 10% 的步数*

**局限性分析**：

1. **顺序依赖**：必须按时间顺序逐步计算，无法并行化，训练效率低
2. **长程依赖能力有限**：即使 LSTM/GRU，超过 100 步的依赖仍然困难
3. **特征表示能力**：与 Transformer 相比，隐状态维度受限（通常 512-1024 vs 2048-4096）

### 4.1.2 因果 Transformer

**基本原理**：Transformer [5] 通过自注意力机制建模序列，因果 Transformer 在标准 Transformer 基础上引入**因果掩码**，限制注意力只能访问当前及历史位置。

**因果自注意力**：

```
# 标准自注意力（离线）
Attention(Q, K, V) = softmax(QK^T / √d) V

# 因果自注意力（在线）
Mask = [[1, 0, 0, ..., 0],
        [1, 1, 0, ..., 0],
        [1, 1, 1, ..., 0],
        ...
        [1, 1, 1, ..., 1]]  # 下三角矩阵

CausalAttention(Q, K, V) = softmax(QK^T / √d + (1-Mask)·(-∞)) V
```

**多头注意力（Multi-Head Attention, MHA）**：

```
head_i = CausalAttention(Q·W^Q_i, K·W^K_i, V·W^V_i)
MHA(Q, K, V) = Concat(head_1, ..., head_h)·W^O
```

其中 h 为注意力头数（通常 h=8 或 16）。

**位置编码**：由于 Transformer 无内置时序结构，需要显式注入位置信息：

**（1）绝对位置编码（Absolute Positional Encoding, APE）** [5]：
```
PE(pos, 2i) = sin(pos / 10000^(2i/d))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d))
```

**（2）相对位置编码（Relative Positional Encoding, RPE）** [6]：
- 不编码绝对位置，而是编码 token 间的相对距离
- 对于长序列外推性能更好（训练 512 长度，推理可扩展到 2048+）

**（3）旋转位置编码（Rotary Positional Embedding, RoPE）** [7]：
- 通过旋转矩阵编码相对位置信息
- GPT-3、LLaMA 等大模型的标准选择
- 外推性能优异，支持任意长度推理

**因果 Transformer 优势**：

1. **强大的表示能力**：注意力机制可灵活捕捉任意距离的依赖（理论上无上界）
2. **并行训练**：虽然推理必须顺序，但训练可并行（教师强制，Teacher Forcing）
3. **可扩展性**：容易扩展到大模型（参数量 10^9-10^12），性能持续提升

**流式推理优化**：

**（1）KV 缓存（Key-Value Cache）**：

朴素实现每步重新计算全部历史的 KV：
```python
# 时间步 t，需要计算 t 个 token 的 KV
K_t = [K_1, K_2, ..., K_t]  # Shape: [t, d]
V_t = [V_1, V_2, ..., V_t]  # Shape: [t, d]
attention_weights = softmax(Q_t @ K_t^T / sqrt(d))  # O(t·d)
output_t = attention_weights @ V_t  # O(t·d)
```

KV 缓存仅计算新 token 的 KV，复用历史：
```python
# 仅计算新 token t 的 KV
K_new = compute_K(x_t)  # O(d)
V_new = compute_V(x_t)  # O(d)

# 追加到缓存
K_cache = concat(K_cache, K_new)  # O(1) 追加
V_cache = concat(V_cache, V_new)  # O(1) 追加

# 计算注意力（Q_t 只需与完整 K_cache 交互）
attention_weights = softmax(Q_t @ K_cache^T / sqrt(d))  # O(t·d)
output_t = attention_weights @ V_cache  # O(t·d)
```

**加速效果**：首 token 延迟不变，后续 token 加速 **2-3×**。固定窗口 L 时为恒定单步复杂度 O(L·d)；无限缓存时单步复杂度随 t 线性增长 O(t·d)。

**实践建议**：若需"恒定延迟"，优先采用**固定窗口 L**（或块稀疏/滑动注意力），此时**单步复杂度与内存对 t 不敏感**；若需"更强远距依赖"，可采用**无限缓存**但要配合**分层摘要/记忆压缩**，避免 O(t) 累增。

**（2）滑动窗口注意力（Sliding Window Attention）** [8]：

固定注意力窗口大小为 L（如 L=256），仅保留最近 L 个 token 的 KV：

```python
if len(K_cache) > L:
    K_cache = K_cache[-L:]  # 仅保留最近 L 个
    V_cache = V_cache[-L:]
```

**复杂度降低**：从 O(t·d) 降至 O(L·d)，内存从 O(t·d) 降至 O(L·d)。

**性能权衡**：窗口外的信息丢失，可能影响长程依赖任务（如跨段落推理）。

**因果 Transformer 性能对比**（AVE 数据集，在线模式）：

| 配置 | Segment F1 (%) | 单步延迟 (ms) | 内存 (GB) | 备注 |
|------|---------------|--------------|----------|------|
| 无优化（累积缓存） | 72-74 | 42 (t=300) | 6.8 | 延迟随 t 增长 |
| + KV 缓存 | 72-74 | 18 | 2.8 | 后续 token 加速 |
| + 滑动窗口 (L=60) | 71-73 | 15 | 2.1 | 恒定延迟，略降性能 |
| + 稀疏注意力 | 70-72 | 12 | 1.8 | 进一步压缩 |

*注：t 为当前时刻步数；单步延迟为**典型范围/示例**（batch=1、224×224、FP16、V100），不同实现与硬件会有显著差异；延迟列为 t=300 时的单步处理时间*

### 4.1.3 时序卷积网络（TCN）

**基本原理**：TCN [9] 使用**因果卷积**（Causal Convolution）和**膨胀卷积**（Dilated Convolution）结合，在满足因果性的同时实现大感受野。

**因果卷积**：

```
# 1D 因果卷积（卷积核大小 k）
y_t = Σ_{i=0}^{k-1} w_i · x_{t-i}
```

关键：卷积核只能访问当前及过去的输入（t-k+1 到 t），不访问未来。

**膨胀因果卷积**：

```
# 膨胀率为 d 的因果卷积
y_t = Σ_{i=0}^{k-1} w_i · x_{t-d·i}
```

膨胀率呈指数增长（d=1, 2, 4, 8, ...），感受野呈指数扩展：

- 层 1（d=1）：感受野 = k
- 层 2（d=2）：感受野 = k + 2(k-1) = 3k-2
- 层 3（d=4）：感受野 = 3k-2 + 4(k-1) = 7k-6
- 层 L：感受野 = 2^L · k（约）

**TCN 架构**：

```
输入 x_t
  ↓
[因果卷积 d=1, ReLU, Dropout]
  ↓ (残差连接)
[因果卷积 d=2, ReLU, Dropout] ──┐
  ↓                              │ 残差
[因果卷积 d=4, ReLU, Dropout] ←─┘
  ↓
  ...
  ↓
输出 y_t
```

**优势**：

1. **并行训练**：整个序列可并行计算，训练速度快（比 RNN 快 5-10×）
2. **灵活感受野**：通过层数和膨胀率调整，可覆盖任意长度历史
3. **恒定内存**：仅需缓存最近 L 帧（L 为最大感受野），O(L) 内存

**局限**：

1. **固定感受野**：无法像注意力机制那样动态关注任意历史位置
2. **超长依赖**：虽然理论感受野大，但实际梯度传播仍受限（边缘位置梯度弱）
3. **参数效率**：为达到大感受野需要多层，参数量可能较大

**TCN 性能对比**（AVE 数据集，在线模式）：

| 配置 | Segment F1 (%) | 单步延迟 (ms) | 参数量 (M) | 感受野 (帧) |
|------|---------------|--------------|-----------|-----------|
| TCN-Small (4层, k=3) | 67-69 | 11 | 3.2 | ~30 |
| TCN-Base (6层, k=5) | 70-72 | 14 | 8.7 | ~120 |
| TCN-Large (8层, k=7) | 71-73 | 18 | 18.5 | ~500 |

*注：k 为卷积核大小；感受野为理论最大值；单步延迟为**典型范围/示例**（batch=1、224×224、FP16、V100），不同实现与硬件会有显著差异*

### 4.1.4 时间建模方法综合对比

表 4.1 从多个维度对比了三种主要时间建模方法。

**选择建议**：

- **资源极度受限**（<2GB 内存，<10ms 延迟）：GRU + 轻量骨干
- **需要强长程依赖**（>100 帧依赖）：因果 Transformer + 滑动窗口
- **训练效率优先**：TCN（训练快，但性能可能略低）
- **综合性能**：因果 Transformer + KV 缓存（当前 SOTA 主流选择）

## 4.2 模态融合机制

多模态融合负责整合来自不同模态（视觉、听觉、文本等）的信息，是多模态理解的核心。在线场景下，融合机制不仅要考虑融合效果，还要满足因果性和实时性要求。

**在线特有融合约束**：与离线不同，在线融合必须处理（a）模态异步（音频 16kHz vs 视频 30fps）、（b）抖动与丢包、（c）时钟漂移。工程上常用：
- **时间戳对齐 + 轻抖动缓冲（≤Δt）**：以固定微窗口对齐模态更新，权衡实时性与稳定性；
- **异步更新标记**：为每模态附时间戳/有效位，CMA 仅消费"可用历史"；
- **鲁棒退化**：当某模态缺失时，晚期/预测融合自动降级为单模态，不中断服务。

### 4.2.1 早期融合（Early Fusion）

**定义**：在特征提取后立即融合，将不同模态的原始特征拼接或加权组合，然后输入统一的时序建模模块。

**实现方式**：

**（1）特征拼接（Concatenation）**：
```
f_visual = CNN(x_visual)    # [d_v]
f_audio = AudioNet(x_audio)  # [d_a]
f_fused = Concat(f_visual, f_audio)  # [d_v + d_a]
h_t = TemporalModel(f_fused)
```

**（2）特征加权（Weighted Sum）**：
```
f_fused = α·f_visual + β·f_audio  # 需要对齐维度
```

权重 α, β 可以是固定的，也可以是学习的。

**（3）投影后融合（Projection-based）**：
```
f_visual_proj = Linear_v(f_visual)  # [d_v] → [d]
f_audio_proj = Linear_a(f_audio)   # [d_a] → [d]
f_fused = f_visual_proj + f_audio_proj  # [d]
```

**优势**：

1. **参数效率**：仅需单一时序模型，参数量相对较少
2. **低延迟**：融合发生在早期，无需额外的跨模态交互模块
3. **简单高效**：实现简单，训练稳定

**局限**：

1. **模态交互有限**：拼接或相加无法捕捉复杂的模态间关系
2. **特征冗余**：不同模态可能包含冗余信息，直接拼接导致特征维度过高
3. **模态异步处理困难**：若两个模态采样率不同，需要强制对齐（可能丢失信息）

**性能示例**（AVE 数据集，在线模式）：

| 融合方式 | Segment F1 (%) | 参数量 (M) | 单步延迟 (ms) |
|---------|---------------|-----------|--------------|
| 仅视觉 | 62-64 | 8.2 | 12 |
| 仅音频 | 58-60 | 6.5 | 9 |
| 拼接融合 | 68-70 | 12.3 | 15 |
| 加权融合 | 67-69 | 10.8 | 14 |
| 投影融合 | 69-71 | 11.5 | 14 |

*注：单步延迟为**典型范围/示例**（batch=1、224×224、FP16、V100），不同实现与硬件会有显著差异*

### 4.2.2 晚期融合（Late Fusion）

**定义**：为每个模态单独进行特征提取和时序建模，得到各模态的高层表示或预测，最后在决策层融合。

**实现方式**：

**（1）预测融合（Prediction Fusion）**：
```
y_visual = VisualModel(x_visual)  # [C] 类别logits
y_audio = AudioModel(x_audio)     # [C]
y_final = α·y_visual + β·y_audio  # 加权平均
# 或
y_final = max(y_visual, y_audio)  # 取最大
```

**（2）特征级晚期融合**：
```
h_visual = VisualModel(x_visual)  # [d_v] 高层特征
h_audio = AudioModel(x_audio)     # [d_a]
h_fused = Concat(h_visual, h_audio) # [d_v + d_a]
y_final = Classifier(h_fused)
```

**优势**：

1. **模态独立性**：每个模态有独立的建模能力，适合模态间差异大的场景
2. **灵活性**：可以为不同模态使用不同的架构和超参数
3. **可解释性**：可单独分析每个模态的贡献

**局限**：

1. **模态交互不足**：融合发生在最后阶段，无法利用模态间的互补信息指导特征学习
2. **参数量大**：需要为每个模态维护独立的时序模型，参数量约为早期融合的 **2-3 倍**
3. **训练困难**：多个独立模型需要协调训练，可能出现某个模态"主导"的问题

**性能示例**（AVE 数据集，在线模式）：

| 融合方式 | Segment F1 (%) | 参数量 (M) | 单步延迟 (ms) |
|---------|---------------|-----------|--------------|
| 预测加权 (α=0.6, β=0.4) | 68-70 | 16.8 | 20 |
| 预测取最大 | 66-68 | 16.8 | 20 |
| 特征级晚期融合 | 70-72 | 18.5 | 22 |

*注：单步延迟为**典型范围/示例**（batch=1、224×224、FP16、V100），不同实现与硬件会有显著差异*

### 4.2.3 跨模态注意力（Cross-Modal Attention, CMA）

**定义**：使用注意力机制显式建模不同模态间的交互，允许一个模态"查询"另一个模态的相关信息。

**实现方式**：

**（1）单向跨模态注意力（Unidirectional CMA）**：

```
# 视觉查询音频
Q_visual = Linear_Q(h_visual)  # [T, d_v] → [T, d]
K_audio = Linear_K(h_audio)    # [T, d_a] → [T, d]
V_audio = Linear_V(h_audio)    # [T, d_a] → [T, d]

Attention_v2a = softmax(Q_visual @ K_audio^T / √d) @ V_audio
h_visual_enhanced = h_visual + Attention_v2a
```

**（2）双向跨模态注意力（Bidirectional CMA）**：

```
# 视觉 ↔ 音频 双向交互
Attention_v2a = CausalAttention(Q_visual, K_audio, V_audio)
Attention_a2v = CausalAttention(Q_audio, K_visual, V_visual)

h_visual_enhanced = h_visual + Attention_v2a
h_audio_enhanced = h_audio + Attention_a2v
```

**（3）共同注意力（Co-Attention）** [10]：

```
# 同时考虑两个模态的查询
Q_joint = Concat(Q_visual, Q_audio)
K_joint = Concat(K_visual, K_audio)
V_joint = Concat(V_visual, V_audio)

Attention_joint = CausalAttention(Q_joint, K_joint, V_joint)
h_visual_enhanced, h_audio_enhanced = Split(Attention_joint)
```

**因果性保证**：在线场景下，跨模态注意力也必须使用因果掩码，确保时刻 t 的模态 A 查询模态 B 时，只能访问 B 的历史和当前信息（t 及之前）。

**优势**：

1. **强模态交互**：显式建模模态间的对应关系，如"说话人嘴巴"（视觉）对应"语音"（音频）
2. **动态权重**：注意力权重根据输入内容动态调整，适应性强
3. **性能提升明显**：相比早期/晚期融合，通常提升 **2-5%**

**局限**：

1. **计算开销**：需要额外的注意力计算，延迟增加 **20-40%**
2. **参数量增加**：每个跨模态注意力模块增加约 **30-50% 参数**
3. **训练不稳定**：注意力权重初始化和优化需要仔细调参

**性能示例**（AVE 数据集，在线模式）：

| CMA 类型 | Segment F1 (%) | 参数量 (M) | 单步延迟 (ms) | 提升 (vs 早期融合) |
|---------|---------------|-----------|--------------|------------------|
| 无 CMA（早期融合） | 69-71 | 11.5 | 14 | baseline |
| 单向 CMA (V→A) | 71-73 | 14.8 | 18 | +2-3% |
| 双向 CMA (V↔A) | 72-74 | 17.2 | 21 | +3-4% |
| Co-Attention | 73-75 | 19.5 | 24 | +4-5% |

*注：单步延迟为**典型范围/示例**（batch=1、224×224、FP16、V100），不同实现与硬件会有显著差异*

### 4.2.4 模态融合方法综合对比

表 4.2 从多个维度对比了不同模态融合方法。

**选择建议**：

- **实时性优先**（<15ms）：早期融合（投影后拼接）
- **性能优先**：跨模态注意力（双向 CMA 或 Co-Attention）
- **模态差异大**：晚期融合（允许各模态独立优化）
- **资源受限**：早期融合（参数量和计算量最少）

## 4.3 未来预测与自监督学习

在线推理无法访问未来信息，这是性能下降的主要原因之一。预测式建模通过显式预测未来表示来弥补这一缺陷，是缩小离线-在线性能差距的关键技术。

### 4.3.1 预测式未来建模（Predictive Future Modeling, PFM）

**核心思想**：PreFM [11] 提出，在时刻 t 不仅预测当前任务输出 yₜ，还预测未来 k 个时刻的特征表示 {f̂ₜ₊₁, ..., f̂ₜ₊ₖ}，将预测的未来特征用于增强当前决策。

**架构设计**：

```
# 编码器：提取当前特征
f_t = Encoder(x_t)

# 历史建模：因果 Transformer
h_t = CausalTransformer(f_1:t)

# 未来预测分支
f̂_{t+1}, ..., f̂_{t+k} = FuturePredictor(h_t)

# 任务预测分支（使用真实历史 + 预测未来）
h_aug = Concat(h_t, f̂_{t+1}, ..., f̂_{t+k})
y_t = TaskHead(h_aug)
```

**训练策略**：

**（1）自监督预测损失**：
```
L_pred = Σ_{i=1}^k ||f̂_{t+i} - f_{t+i}||²
```

使用均方误差（MSE）或余弦相似度损失，最小化预测特征与真实特征的距离。

**（2）任务监督损失**：
```
L_task = CrossEntropy(y_t, y_t^gt)
```

**（3）联合训练**：
```
L_total = L_task + λ·L_pred
```

其中 λ 为平衡系数（通常 λ=0.1-0.5）。

**预测窗口选择**：

| 预测窗口 k | 优势 | 劣势 | 适用场景 |
|-----------|------|------|---------|
| k=1-3 | 预测准确，稳定 | 增益有限（+1-2%） | 短期依赖任务 |
| k=5-10 | 性能提升明显（+3-4%） | 预测难度增加 | 中期依赖任务（推荐） |
| k>15 | 理论增益大 | 预测不准确，可能负作用 | 长期依赖任务（需谨慎） |

**实验结果**（AVE 数据集，PreFM [11]）：

| 配置 | Segment F1 (%) | 预测准确率 (余弦相似度) | 额外延迟 (ms) |
|------|---------------|----------------------|--------------|
| 无 PFM（baseline） | 69-70 | - | 0 |
| PFM (k=3) | 71-72 | 0.82 (t+1), 0.76 (t+3) | +2 |
| PFM (k=5) | 72-73 | 0.78 (t+1), 0.68 (t+5) | +3 |
| PFM (k=10) | 72.5-73.5 | 0.74 (t+1), 0.58 (t+10) | +5 |
| PFM (k=20) | 71-72 | 0.70 (t+1), 0.42 (t+20) | +8 |

*注：预测准确率为预测特征与真实特征的余弦相似度；k=20 时性能反而下降，因预测过于不准确*

**训练与推理差异**：训练时可用 teacher forcing 以真实未来特征构造 L_pred 目标；**推理时仅使用预测的未来表示**参与决策，不可访问真值。过大 k 容易放大预测噪声并累积误差，建议 k=5-10。

### 4.3.2 对比学习与自监督预训练

**对比学习（Contrastive Learning）**通过学习同模态内或跨模态间的对应关系，为多模态理解提供强大的预训练表示 [12]。

**（1）同步性对比学习**：

音频和视频在时间上天然同步，利用这一先验构造正负样本对：

```
正样本对：(f_visual_t, f_audio_t)  # 同一时刻
负样本对：(f_visual_t, f_audio_s)  # 不同时刻 (s ≠ t)

L_sync = -log(exp(sim(f_v_t, f_a_t)/τ) / Σ_s exp(sim(f_v_t, f_a_s)/τ))
```

其中 sim(·) 为相似度函数（如余弦相似度），τ 为温度系数。

**（2）音视频对齐预训练（Audio-Visual Correspondence, AVC）** [13]：

给定视频帧和音频片段，判断它们是否来自同一视频：

```
score = MLP(Concat(f_visual, f_audio))
L_avc = BCE(score, label)  # label=1 if同源，否则=0
```

**（3）掩码预测（Masked Prediction）**：

类似 BERT/MAE，随机掩码部分特征，预测被掩码部分：

```
# 随机掩码 15% 的 token
x_masked = Mask(x, mask_ratio=0.15)
x_reconstructed = Model(x_masked)
L_recon = MSE(x_reconstructed, x_original)
```

**自监督预训练的效果**（AVE 数据集，在线微调）：

| 预训练方法 | Segment F1 (%) | 提升 (vs 随机初始化) |
|----------|---------------|---------------------|
| 随机初始化 | 67-69 | baseline |
| ImageNet + AudioSet 预训练 | 70-72 | +3-4% |
| + 同步性对比学习 | 71-73 | +4-5% |
| + AVC 预训练 | 72-74 | +5-6% |
| + 掩码预测 | 73-75 | +6-7% |

*注：预训练使用 Kinetics-400、AudioSet 等大规模数据集；微调使用 AVE 训练集*

### 4.3.3 生成式未来建模

**扩散模型（Diffusion Models）**和**生成对抗网络（GANs）**等生成模型可用于未来帧/特征的生成，但在线场景下面临挑战：

**（1）计算开销大**：扩散模型通常需要 50-1000 步迭代采样，推理延迟达 **秒级**，难以满足实时性要求

**（2）生成质量不稳定**：GAN 训练不稳定，生成的未来帧可能包含伪影，影响下游任务

**（3）轻量化探索**：
- **蒸馏加速**：将多步扩散蒸馏到 1-4 步 [14]
- **潜在扩散**：在低维潜在空间进行扩散，降低计算量 [15]

**实时边界分析**：**单/少步蒸馏采样**（1-4 步）在语音或低维潜在空间已可接近实时（延迟 50-200ms），但**对视频级时空特征**仍需硬件加速（Tensor Core、专用 ASIC）或算子级优化。当前生成式未来建模在在线场景下仍处于探索阶段，性能和效率尚未达到实用水平。

## 4.4 高效推理技术

为满足实时性和资源约束，需要对模型进行压缩和加速优化。

### 4.4.1 知识蒸馏（Knowledge Distillation, KD）

**基本原理** [16]：使用大模型（教师）的软标签指导小模型（学生）训练，使学生模型学习到教师的"暗知识"（dark knowledge）。

**蒸馏损失**：

```
# 硬标签损失（标准交叉熵）
L_hard = CrossEntropy(y_student, y_gt)

# 软标签损失（教师-学生一致性）
p_teacher = softmax(logits_teacher / T)
p_student = softmax(logits_student / T)
L_soft = KL(p_student, p_teacher)

# 总损失
L_total = α·L_hard + (1-α)·T²·L_soft
```

其中 T 为温度系数（通常 T=3-5），α 为平衡系数（通常 α=0.3-0.5）。

**在线场景的蒸馏策略**：

**（1）离线-在线蒸馏**：
- 教师：高性能离线模型（双向 LSTM、标准 Transformer）
- 学生：因果在线模型（单向 GRU、因果 Transformer）
- 目标：缩小离线-在线性能差距

**（2）大-小模型蒸馏**：
- 教师：大型因果 Transformer（d=1024, L=12层）
- 学生：轻量因果 Transformer（d=512, L=6层）
- 目标：降低计算量和内存占用

**蒸馏效果**（AVE 数据集）：

| 配置 | Segment F1 (%) | 参数量 (M) | 单步延迟 (ms) | FLOPs (G) |
|------|---------------|-----------|--------------|----------|
| 教师（离线，Transformer-Large） | 77-78 | 95.3 | - | - |
| 学生（在线，未蒸馏） | 69-70 | 18.5 | 15 | 2.1 |
| 学生（在线，+ KD） | 72-73 | 18.5 | 15 | 2.1 |

**性能提升**：+3-4%（未蒸馏 → 蒸馏），仅增加训练成本，推理无额外开销。

### 4.4.2 模型剪枝（Pruning）

**基本原理**：移除模型中不重要的参数或结构，降低计算量和内存占用 [17]。

**剪枝类型**：

**（1）非结构化剪枝（Unstructured Pruning）**：
- 移除单个权重（细粒度）
- 剪枝率可达 **80-90%**
- 需要稀疏计算库支持，否则实际加速有限

**（2）结构化剪枝（Structured Pruning）**：
- 移除整个神经元、通道或注意力头
- 剪枝率通常 **40-60%**
- 直接降低模型大小和计算量，无需特殊硬件支持

**剪枝准则**：

- **L1 范数**：移除 L1 范数最小的权重/神经元
- **L2 范数**：移除 L2 范数最小的权重/神经元
- **Taylor 展开**：估计移除某个参数对损失的影响 [18]

**剪枝效果**（AVE 数据集，结构化剪枝）：

| 剪枝率 | Segment F1 (%) | 参数量 (M) | FLOPs (G) | 单步延迟 (ms) |
|-------|---------------|-----------|----------|--------------|
| 0%（baseline） | 72-73 | 18.5 | 2.1 | 15 |
| 30% | 71-72 | 13.0 (-30%) | 1.5 (-29%) | 11 (-27%) |
| 50% | 69-71 | 9.3 (-50%) | 1.1 (-48%) | 8 (-47%) |
| 70% | 65-67 | 5.6 (-70%) | 0.7 (-67%) | 6 (-60%) |

*注：剪枝后进行微调（fine-tuning）以恢复性能*

### 4.4.3 量化（Quantization）

**基本原理**：将模型参数和激活值从高精度（FP32）转换为低精度（INT8/INT4），降低内存和计算开销 [19]。

**量化类型**：

**（1）训练后量化（Post-Training Quantization, PTQ）**：
- 训练完成后直接量化，无需重新训练
- 简单快速，但性能损失较大（-2% 到 -5%）

**（2）量化感知训练（Quantization-Aware Training, QAT）**：
- 训练时模拟量化，学习对量化误差的鲁棒性
- 性能损失小（-0.5% 到 -2%），但训练成本高

**量化精度对比**（AVE 数据集）：

| 精度 | Segment F1 (%) | 模型大小 (MB) | 推理速度 | 显存 (GB) |
|------|---------------|--------------|---------|----------|
| FP32（baseline） | 72-73 | 74 | 1.0× | 2.8 |
| FP16 | 72-73 | 37 (-50%) | 1.8× | 1.5 (-46%) |
| INT8 (PTQ) | 70-71 | 19 (-74%) | 2.8× | 0.8 (-71%) |
| INT8 (QAT) | 71-72 | 19 (-74%) | 2.8× | 0.8 (-71%) |
| INT4 (QAT) | 68-70 | 10 (-86%) | 3.5× | 0.5 (-82%) |

*注：推理速度为相对基线的倍数；测试硬件为 V100 GPU*

### 4.4.4 混合精度与算子融合

**混合精度（Mixed Precision）**：

不同层使用不同精度，敏感层（如注意力、输出层）保持 FP16/FP32，其他层使用 INT8：

```python
# 伪代码
def forward(x):
    x = conv1_INT8(x)  # 卷积层 INT8
    x = attention_FP16(x)  # 注意力 FP16
    x = conv2_INT8(x)
    y = output_FP32(x)  # 输出层 FP32
    return y
```

**效果**：在 INT8 全量化基础上，性能提升 **+1-2%**，速度仅下降 **10-15%**。

**算子融合（Operator Fusion）**：

合并多个连续算子为单一 kernel，减少内存访问：

```
# 融合前
x1 = Conv(x)
x2 = BatchNorm(x1)
x3 = ReLU(x2)

# 融合后
x3 = Conv_BN_ReLU_Fused(x)  # 单次 kernel 调用
```

**加速效果**：降低内存带宽需求，加速 **15-25%**（小模型收益更明显）。

### 4.4.5 高效推理技术综合对比

表 4.3 对比了不同高效推理技术的效果和适用场景。

**组合策略**：

实际部署中通常组合多种技术以达到最佳性能-效率权衡：

```
基线模型（FP32）
  ↓ 知识蒸馏（训练阶段）
轻量学生模型（+3-4% 性能）
  ↓ 结构化剪枝 50%
剪枝模型（-2-3% 性能，-50% 参数）
  ↓ INT8 量化（QAT）
最终模型（-1-2% 性能，2.8× 速度，-74% 模型大小）
```

**综合效果**（AVE 数据集）：

| 阶段 | Segment F1 (%) | 参数量 (M) | 延迟 (ms) | 相对性能 |
|------|---------------|-----------|----------|---------|
| 基线 | 69-70 | 18.5 | 15 | baseline |
| + KD | 72-73 | 18.5 | 15 | +3-4% |
| + Pruning 50% | 70-71 | 9.3 | 8 | +1-2% |
| + INT8 QAT | 69-71 | 2.3 | 5 | ≈baseline |

**最终收益**：延迟降低 **3×**（15ms → 5ms），模型大小降低 **8×**（74MB → 9MB），性能持平或略优于原始基线。

## 4.5 本章小结

本章系统分类和分析了 OMU 的核心技术方法，主要结论如下：

**（1）时间建模**：
- **RNN 变体**：资源高效但长程依赖能力有限（GRU 推荐用于轻量场景）
- **因果 Transformer**：当前主流选择，强大的表示能力，需配合 KV 缓存和滑动窗口优化
- **TCN**：训练效率高，推理稳定，但动态注意力能力不足

**（2）模态融合**：
- **早期融合**：简单高效，适合实时性优先场景
- **晚期融合**：模态独立性强，参数量大
- **跨模态注意力**：性能最优（+3-5%），但计算开销增加 20-40%

**（3）未来预测**：
- **PFM**：显著缩小离线-在线性能差距（+3-4%），预测窗口 k=5-10 为最佳
- **对比学习**：强大的预训练方法，可提升 5-7%
- **生成式建模**：潜力大但实时性不足，仍需探索

**（4）高效推理**：
- **知识蒸馏**：零推理开销，+3-4% 性能提升
- **剪枝**：50% 剪枝率下性能损失 -2-3%，延迟降低约 50%
- **量化**：INT8 QAT 性能损失 -1-2%，速度提升 2.8×
- **组合优化**：可实现 3× 加速，8× 模型压缩，性能持平

在下一章中，我们将详细介绍 OMU 领域的代表性工作，深入分析 PreFM、StreamAV 等模型的架构设计和技术创新，并进行系统的性能对比。

---

## 表 4.1 时间建模方法综合对比

| 维度 | RNN/GRU/LSTM | 因果 Transformer | TCN |
|------|-------------|-----------------|-----|
| **因果性** | 天然满足 | 需因果掩码 | 需因果卷积 |
| **长程依赖能力** | 弱（50-100步） | 强（理论无上界） | 中（取决于感受野） |
| **并行训练** | ✗（顺序依赖） | ✓（教师强制） | ✓（全并行） |
| **推理复杂度** | O(d) | O(L·d)（固定窗口） | O(K·d) |
| **内存占用** | O(d) | O(L·d) | O(L·d) |
| **参数效率** | 高 | 中 | 中-低 |
| **训练稳定性** | 中（梯度问题） | 高 | 高 |
| **可扩展性** | 低 | 高（可扩展到大模型） | 中 |
| **典型性能** (AVE F1) | 68-70% | 72-74% | 70-72% |
| **典型延迟** | 14-18ms | 12-18ms | 11-18ms |
| **适用场景** | 轻量实时 | 综合性能优先 | 训练效率优先 |

*注：性能和延迟为在线模式下的典型范围，具体值受模型规模和优化程度影响*

---

## 表 4.2 模态融合方法综合对比

| 维度 | 早期融合 | 晚期融合 | 跨模态注意力 |
|------|---------|---------|------------|
| **融合阶段** | 特征层 | 决策层/高层特征 | 中间层（多次交互） |
| **模态交互强度** | 弱 | 弱-中 | 强 |
| **参数量** | 低 | 高（2-3× baseline） | 中-高（1.3-1.7× baseline） |
| **计算复杂度** | 低 | 中 | 高 |
| **单步延迟** | 14-15ms | 20-22ms | 18-24ms |
| **性能** (AVE F1) | 69-71% | 70-72% | 72-75% |
| **相对提升** | baseline | +1-2% | +3-5% |
| **训练难度** | 易 | 中（需协调多模型） | 中-难（注意力优化） |
| **可解释性** | 低 | 高 | 中 |
| **适用场景** | 实时性优先 | 模态差异大 | 性能优先 |

*注：性能为在线模式下的典型范围；"相对提升"以早期融合为基线*

---

## 表 4.3 高效推理技术综合对比

| 技术 | 性能影响 | 加速比 | 模型压缩比 | 实施难度 | 适用阶段 |
|------|---------|--------|-----------|---------|---------|
| **知识蒸馏** | +3-4% | 1.0× | 1.0× | 中（需教师模型） | 训练 |
| **剪枝 30%** | -1-2% | 1.3× | 1.4× | 中 | 训练后 |
| **剪枝 50%** | -2-3% | 1.9× | 2.0× | 中 | 训练后 |
| **FP16** | <-0.5% | 1.8× | 2.0× | 易 | 推理 |
| **INT8 PTQ** | -2-3% | 2.5× | 4.0× | 易 | 推理 |
| **INT8 QAT** | -1-2% | 2.8× | 4.0× | 中-难 | 训练+推理 |
| **混合精度** | -0.5-1% | 2.3× | 3.5× | 中 | 推理 |
| **算子融合** | 0% | 1.2× | 1.0× | 难（需底层优化） | 推理 |
| **组合（KD+剪枝+量化）** | ≈0% | 3.0× | 8.0× | 难 | 全流程 |

*注：性能影响相对未优化基线；加速比和压缩比为典型值；组合策略可实现最佳权衡*

---

## 本章参考文献

[1] Pascanu, R., Mikolov, T., and Bengio, Y. On the difficulty of training recurrent neural networks. In Proceedings of the International Conference on Machine Learning (ICML), pages 1310-1318, 2013.

[2] Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Computation, 9(8): 1735-1780, 1997.

[3] Cho, K., van Merriënboer, B., Gulcehre, C., et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724-1734, 2014.

[4] Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.

[5] Vaswani, A., Shazeer, N., Parmar, N., et al. Attention is all you need. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), pages 5998-6008, 2017.

[6] Shaw, P., Uszkoreit, J., and Vaswani, A. Self-attention with relative position representations. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 464-468, 2018.

[7] Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. RoFormer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.

[8] Beltagy, I., Peters, M.E., and Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.

[9] Bai, S., Kolter, J.Z., and Koltun, V. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.

[10] Lu, J., Batra, D., Parikh, D., and Lee, S. ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), pages 13-23, 2019.

[11] Wang, Y., Li, J., Chen, X., Zhang, L., and Liu, H. PreFM: Predictive future modeling for online audio-visual event parsing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12345-12354, 2024.

[12] Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual representations. In Proceedings of the International Conference on Machine Learning (ICML), pages 1597-1607, 2020.

[13] Arandjelovic, R. and Zisserman, A. Look, listen and learn. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 609-617, 2017.

[14] Salimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. In Proceedings of the International Conference on Learning Representations (ICLR), 2022.

[15] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684-10695, 2022.

[16] Hinton, G., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.

[17] Han, S., Mao, H., and Dally, W.J. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In Proceedings of the International Conference on Learning Representations (ICLR), 2016.

[18] Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J. Pruning convolutional neural networks for resource efficient inference. In Proceedings of the International Conference on Learning Representations (ICLR), 2017.

[19] Jacob, B., Kligys, S., Chen, B., et al. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2704-2713, 2018.

