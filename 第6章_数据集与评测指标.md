# 第 6 章 数据集与评测指标

在第 5 章中，我们详细介绍了在线多模态理解（OMU）领域的代表性工作，分析了 PreFM、StreamAV 等模型的架构设计和性能特征。本章将系统介绍 OMU 任务常用的数据集和评测指标，分析不同数据集的特点、挑战，并重点讨论在线推理环境下的评估方法和特殊要求。

## 6.1 音视频事件解析数据集

**术语对照**：
- **AVEL**：Audio-Visual Event Localization（音视频事件定位，侧重时间边界）
- **AVEP**：Audio-Visual Event Parsing（事件解析，含类别与时间）
- **AVVP**：Audio-Visual Video Parsing（弱监督解析，常含模态可见性）

*本章表格与正文统一采用上述缩写。*

### 6.1.1 AVE 数据集（Audio-Visual Event）

**基本信息** [1]：
- **提出者**：Tian et al., ECCV 2018
- **规模**：4,143 个未剪辑视频
- **事件类别**：28 类（如婴儿哭、汽车鸣笛、狗叫、钢琴演奏等）
- **视频长度**：每个视频约 10 秒（平均 295 帧 @30fps）
- **标注方式**：片段级（1 秒粒度）时间边界标注
- **标注内容**：事件类别 + 事件发生的时间段

**数据来源**：YouTube 视频，覆盖日常生活、动物、乐器、工具等多种场景。

**标注示例**：
```
视频 ID: video_001.mp4 (10秒)
标注：
  [0-2s]: 汽车鸣笛 (car horn)
  [3-5s]: 背景噪音 (background)
  [6-8s]: 狗叫 (dog bark)
  [8-10s]: 背景噪音 (background)
```

**数据分布**：
- **训练集**：3,339 视频（约 80%）
- **测试集**：402 视频（约 10%）
- **验证集**：402 视频（约 10%）

**类别分布**：
- 平均每个类别：约 **148 个视频**
- 类别不平衡性：最多类别（狗叫）约 250 个视频，最少类别约 80 个视频
- 类别不平衡比：约 **3.1:1**（相对温和）

**特点与挑战**：

**（1）片段级细粒度标注**：提供 1 秒粒度的时间边界，适合评估时序定位能力。

**（2）事件稀疏性**：约 **35-40%** 的片段标注为背景（无事件），模型需要区分事件与背景。

**（3）单标签假设**：每个片段通常仅包含一个主要事件，不支持多事件重叠场景。

**（4）音视频对齐良好**：YouTube 视频通常音视频同步，未包含严重的模态异步挑战。

**在线评测适用性**：
- ✅ 片段级标注适合逐帧在线评测
- ✅ 视频长度适中（10 秒），测试快速
- ⚠️ 缺乏模态可见性标注（仅有事件类别）
- ⚠️ 未包含模态异步/缺失等在线特有挑战

*注：AVE 常用于 AVEL（定位）或 AVEP（解析）任务设定，本文默认采用片段级定位（AVEL）口径。*

### 6.1.2 LLP 数据集（Look, Listen, and Parse）

**基本信息** [2]：
- **提出者**：Tian et al., ECCV 2020
- **规模**：约 11,000 个未剪辑视频
- **事件类别**：约 25 类（与 AVE 部分重叠，但增加了模态可见性维度）
- **视频长度**：每个视频约 10 秒
- **标注方式**：弱监督（仅视频级标签）
- **标注内容**：事件类别 + **模态可见性**（audio-only / visual-only / audio-visual / background）

**模态可见性示例**：
```
视频 ID: video_002.mp4
标注（视频级）：
  事件：钢琴演奏 (piano)
  模态可见性：
    - 片段1: audio-visual（可见+可听）
    - 片段2: visual-only（仅可见，无声）
    - 片段3: audio-only（仅可听，画面为其他）
```

**数据分布**：
- **训练集**：约 8,800 视频（约 80%）
- **测试集**：约 1,100 视频（约 10%）
- **验证集**：约 1,100 视频（约 10%）

**模态可见性分布**（基于 [2] 的统计）：
- Audio-visual：约 **55-60%** 片段
- Visual-only：约 **15-20%** 片段
- Audio-only：约 **10-15%** 片段
- Background：约 **15-20%** 片段

**特点与挑战**：

**（1）模态可见性标注**：首个大规模包含模态可见性标注的数据集，支持更细粒度的音视频理解。

**（2）弱监督设定**：仅提供视频级标签（不提供精确时间边界），增加了学习难度，更贴近实际应用（标注成本低）。

**（3）更大规模**：视频数约为 AVE 的 **2.6 倍**，类别分布更均衡。

**（4）模态解耦挑战**：需要模型自动发现哪些片段包含视觉/听觉信息，测试模态分离能力。

**在线评测适用性**：
- ✅ 模态可见性标注适合评测在线融合策略
- ✅ 弱监督设定更贴近实际应用
- ⚠️ 缺少精确时间边界（需要定位-分类联合评估）
- ⚠️ 类别数与清洗策略在不同实现中略有差异（23-25 类）

### 6.1.3 UnAV-100 数据集

**基本信息** [3]：
- **提出者**：Lin et al., ICCV 2021
- **规模**：33,924 个 10 秒视频片段
- **事件类别**：100 类（更细粒度分类）
- **视频长度**：每个片段 10 秒
- **标注方式**：**未配对**音视频（音频与视频来自不同视频，故意不对齐）
- **设计目的**：评估无监督音视频对齐和学习能力

**数据构造**：
```
原始同步视频 → 随机打乱音频轨道 → 未配对音视频片段
                                    ↓
                          模型任务：学习重新配对
```

**特点与挑战**：

**（1）未配对设计**：音视频刻意不对齐，需要模型学习音视频对应关系。

**（2）更多类别**：100 类事件，覆盖更广泛的场景（人类活动、动物、自然环境、乐器等）。

**（3）对齐学习任务**：主要用于对比学习预训练、音视频对应性学习（AVC），而非直接的事件解析。

**（4）大规模**：片段数是 AVE 的 **8.2 倍**，适合预训练。

**在线评测适用性**：
- ✅ 适合评测在线对齐能力（模态异步场景）
- ✅ 大规模，适合预训练
- ⚠️ 主要用于预训练，直接事件解析需额外适配
- ⚠️ 未配对设计不反映真实在线场景（实际音视频是对齐的）

**任务定位说明**：UnAV-100 主要用于 **AVC（Audio-Visual Correspondence，音视频对应性）** 或**同步性对比学习**预训练；若转为 AVEL/AVEP 任务需额外构造片段级或视频级事件标签，目前无官方 AVEP 标注版本。

### 6.1.4 其他相关数据集

**ActivityNet Captions** [4]：
- **规模**：20,000 个 YouTube 视频
- **平均长度**：180 秒（远长于 AVE/LLP）
- **任务**：视频描述生成、时序动作定位
- **在线挑战**：长视频需要历史摘要和记忆管理

**Kinetics-400/700** [5]：
- **规模**：400 类（Kinetics-400）或 700 类（Kinetics-700），约 30-65 万视频片段
- **任务**：动作识别（单标签分类）
- **用途**：主要用于预训练视觉编码器（I3D、SlowFast 等）
- **在线相关性**：可用于评测在线动作识别，但原始标注未包含模态可见性

**AudioSet** [6]：
- **规模**：200 万个 10 秒音频片段，632 类音频事件
- **任务**：音频事件检测（多标签分类）
- **用途**：预训练音频编码器（VGGish、PANNs）
- **在线相关性**：纯音频，不包含视觉信息

**数据集对比总结**（见表 6.1）。

## 6.2 评测指标体系

**测量口径说明**：除非特别说明，本章在线延迟/吞吐量/显存等数值为**典型范围或示例**，默认测试条件为 `batch=1、输入224×224、FP16（若适用）、NVIDIA V100`。具体实现（分辨率、骨干、优化、硬件）将显著影响绝对值。

### 6.2.1 离线评测指标

**（1）Segment-level F1（片段级 F1）**

**定义**：将视频按固定窗口（通常 1s）切分为片段，对每个片段进行多类或多标签判断。以**类别层面的 TP/FP/FN**为基础计算 Precision/Recall 与 F1。默认报告**微平均（Micro-F1）**，即先在所有类别与片段上累计 TP/FP/FN，再计算 F1；如需类别均衡，可同时报告**宏平均（Macro-F1）**。

**计算方式（Micro-F1）**：

```
TP = FP = FN = 0
for each class c in classes:
    for each segment i:
        y = gt[i][c]       # 0/1（或 one-hot）
        p = pred[i][c]     # 0/1（或阈值后）
        TP += (p==1 and y==1)
        FP += (p==1 and y==0)
        FN += (p==0 and y==1)

Precision = TP / (TP + FP + 1e-9)
Recall    = TP / (TP + FN + 1e-9)
F1_micro  = 2 * Precision * Recall / (Precision + Recall + 1e-9)
```

**Macro-F1**：对每个类别 c 单独计算 F1_c，然后取平均：`F1_macro = mean_c(F1_c)`。

**适用场景**：
- AVE 数据集的主要指标
- 适合单标签或主标签场景
- 默认使用**微平均**（Micro-F1），除非特别说明

**阈值与校准**：
- **多标签阈值**：可采用固定阈值（如 0.5）、验证集独立扫描最优阈值（per-class）、或 Top-K 选择
- **阈值确定原则**：阈值应在**验证集**上确定，**测试集**固定不变，避免过拟合
- **建议报告**：PR 曲线（Precision-Recall curve）与 mAP@全阈值，以展示模型在不同工作点的性能
- **在线门控影响**：若使用早停或置信度门控，应说明是否引入滞回（hysteresis）以避免抖动

**（2）Event-wise mAP（mean Average Precision）**

**定义**：将视频内各事件类别作为多标签检测任务，按类别计算 AP 后取均值。

**口径说明**：除非特别说明，本文采用 **VOC-11 点插值** 计算 AP_c（阈值集合 {0, 0.1, ..., 1.0}），再对类别取均值得到 mAP。若使用 COCO 风格（逐点积分/多 IoU），需在文中显式标注，并不得与 VOC 口径直接横比。

**计算方式**：

```
# 对每个类别 c
# 按预测置信度排序所有预测
predictions_c = sort_by_confidence(all_predictions[c])

# 计算 Precision-Recall 曲线
for threshold in thresholds:
    TP, FP, FN = count(predictions_c, threshold)
    precision = TP / (TP + FP)
    recall = TP / (TP + FN)

# 计算 AP（曲线下面积）
AP_c = integral(precision(recall))

# 计算 mAP
mAP = mean([AP_1, AP_2, ..., AP_K])
```

**适用场景**：
- LLP 数据集的主要指标
- 适合多标签、弱监督场景
- 对类别不平衡更鲁棒

**（3）其他离线指标**

**Frame-level Accuracy**：
- 逐帧评估准确率（更细粒度，但对时间边界误差敏感）
- 计算：Acc = correct_frames / total_frames

**IoU-based mAP**：
- 引入时间 IoU（Intersection over Union）阈值（如 IoU ≥ 0.5）
- 仅当预测片段与真值片段的时间重叠度 ≥ 阈值时才算正确
- 适合时序动作定位任务（ActivityNet）

### 6.2.2 在线评测指标

在线评测不仅要关注准确率，还要综合评估延迟、吞吐量、鲁棒性等多个维度。

**（1）延迟容忍准确率（Latency-Tolerant Accuracy, Acc@Δt）**

**定义**（已在第 3 章引入）：若预测在允许的时间窗内到达且标签正确，则视为命中。

**时间基准**：以模型输出时间戳 t_pred 与片段时间戳 t_seg 比较；延迟定义为 delay = t_pred - t_seg。若使用墙钟时间需说明同步方式。

**两种计分模式**：
- **Strict**：仅当 `0 ≤ delay ≤ Δt` 且预测正确，计为命中（不奖励早报）。
- **Early-ok**：当 `|delay| ≤ Δt` 且预测正确，计为命中（允许早报窗口）。

**形式化（Strict）**：
```
Acc@Δt = (1/T) Σₜ 𝟙[yₜ^pred == yₜ^gt  ∧  0 ≤ (t_pred - t) ≤ Δt]
```

报告建议：同时给出 Δt ∈ {0, 50, 100, 200, 500, 1000} ms 的曲线；若采用 Early-ok，请在图例中标注。

**F1@Δt 计算**：先用 Δt 规则筛选满足延迟要求的预测片段，将其作为 TP/FP 候选，然后按标准 Micro-F1 口径聚合计算（而非简单准确率）。图 6.1 展示的曲线为 **F1@Δt（Strict 模式）**。

**不确定性与校准**（可选指标）：
- **ECE（Expected Calibration Error）**：评估预测置信度的校准程度，适用于报警阈值设置
- **Brier Score**：概率预测与真值的均方误差，越低越好

**不同 Δt 设定的意义**：
- **Δt = 0ms**：严格实时，不允许任何延迟（最严格）
- **Δt = 100ms**：允许人眼难以察觉的微小延迟（常用于监控）
- **Δt = 500ms**：允许半秒延迟（接近离线性能）
- **Δt = 1000ms**：允许 1 秒延迟（宽松设定）

**评测建议**：应报告**多个 Δt 下的性能曲线**，而非单一数值：

| Δt (ms) | Segment F1 (%) | 说明 |
|---------|---------------|------|
| 0 | 72-73 | 严格实时 |
| 50 | 72.5-73.5 | 微延迟 |
| 100 | 73-74 | 人眼难察觉 |
| 500 | 73.5-74.5 | 接近离线 |

**（2）实时性指标（Real-time Metrics）**

**单帧延迟（Per-frame Latency）**：
```
Latency_avg = (1/T) Σₜ time(process_frame_t)
Latency_max = max_t time(process_frame_t)
Latency_p95 = 95th_percentile(latencies)
```

建议报告**平均、最大、P95**三个统计量（最大延迟反映最坏情况，P95 更鲁棒）。

**吞吐量（Throughput）**：
```
FPS = 1 / Latency_avg
```

**实时因子（Real-Time Factor, RTF）**：
```
RTF = 处理时长 / 视频实际时长
```

- RTF < 1：快于实时（如 RTF=0.5 表示可处理 2× 速度视频）
- RTF = 1：刚好实时
- RTF > 1：慢于实时（无法满足实时要求）

**示例**：处理 30fps 视频，单帧延迟 15ms：
```
FPS = 1000ms / 15ms = 66.7
RTF = 15ms / 33.3ms = 0.45（快于实时）
```

*注：RTF < 1 ⇒ 可实时；RTF × fps_in = fps_out（输出帧率）*

**计时与稳定性建议**：
- 评测前进行 ≥100 帧**预热**
- 计时**不包含数据加载/I-O**
- 报告 `Latency_avg / Latency_p50 / Latency_p95 / Latency_p99 / Latency_max`
- 固定随机种子与线程绑定（如 `torch.set_num_threads(1)`）
- GPU 计时使用 **CUDA events（启用同步）**，CPU 使用 `time.perf_counter()`
- 跨设备需**时钟同步**，否则不要混用时间戳
- 在附录给出完整环境信息（见下方环境锁定清单）

**环境锁定清单**（建议在附录给出）：
- 驱动版本（如 NVIDIA Driver 515.65）
- CUDA 版本（如 11.7）
- cuDNN 版本（如 8.5.0）
- 深度学习框架与版本（如 PyTorch 2.0.1）
- GPU 型号与数量（如 1× NVIDIA V100 32GB）
- GPU 功耗上限（如 300W，默认或限制）
- GPU 时钟模式（Persistent mode 开启/关闭）
- 线程/OMP 环境变量（如 `OMP_NUM_THREADS=1`）
- 数据解码库（如 FFmpeg 4.4，是否异步解码）

**（3）在线鲁棒性指标**

在第 5 章中，我们引入了三类在线鲁棒性指标，本节详细定义其评测协议。

**Missing-Rate@p%**（模态缺失容忍度）：
```
# 评测协议
1. 随机选择 p% 的时间步
2. 在这些步骤中，将某个模态的输入置零或移除
3. 模型需处理缺失，不应崩溃
4. 报告 F1 下降幅度
```

**典型评测点**：p = 10%, 20%, 30%, 50%

**示例结果**：
| Missing Rate | StreamAV F1 (%) | PreFM F1 (%) |
|--------------|----------------|-------------|
| 0% (正常) | 73-74 | 72-73 |
| 10% 视觉缺失 | 71-72 (-2-3%) | 68-69 (-4-5%) |
| 30% 视觉缺失 | 66-68 (-7-9%) | 61-63 (-11-13%) |

**Audio-Delay@Δt**（音频延迟容忍度）：
```
# 评测协议
1. 对音频模态施加固定延迟 +Δt（如 +50ms, +100ms, +200ms）
2. 模拟传输延迟或处理延迟
3. 模型需在异步条件下融合
4. 报告 F1 变化
```

**典型评测点**：Δt = 0ms, 50ms, 100ms, 200ms, 500ms

**Jitter-Tolerance@±δ**（时基抖动容忍度）：
```
# 评测协议
1. 对每帧的时间戳施加随机抖动：ts' = ts + random(-δ, +δ)
2. 模拟时钟不精确或采样率抖动
3. 模型需在抖动条件下对齐模态
4. 报告 F1 变化
```

**典型评测点**：δ = 0ms, 10ms, 20ms, 50ms

### 6.2.3 在线评测协议

**标准在线评测流程**：

```python
# 伪代码
def evaluate_online(model, dataset):
    latencies = []
    predictions = []
    
    for video in dataset:
        # 初始化
        model.reset_state()
        
        # 逐帧推理
        for t, frame in enumerate(video):
            start_time = timer()
            
            # 在线推理（仅访问历史）
            pred_t = model.predict(frame)
            
            end_time = timer()
            latency_t = end_time - start_time
            
            # 记录
            latencies.append(latency_t)
            predictions.append(pred_t)
        
    # 计算指标
    F1 = compute_segment_f1(predictions, ground_truth)
    Latency_avg = mean(latencies)
    Latency_p95 = percentile(latencies, 95)
    FPS = 1000 / Latency_avg
    
    return {
        'F1': F1,
        'Latency_avg': Latency_avg,
        'Latency_p95': Latency_p95,
        'FPS': FPS
    }
```

**实现细节约定**：
- **批大小**：`batch=1` 强制；不允许通过堆叠片段"伪实时"。
- **状态管理**：每个视频（或每条流）`reset_state()`；禁止跨视频共享缓存。
- **计时点**：以 `predict()` 函数边界计时（含模型前后处理，不含解码/I-O）。
- **随机抖动**：对 Jitter/Timestamp 模拟应记录随机种子，结果可复现。

**关键要求**：

1. **因果性验证**：
   - 方法一：代码审查，确认不访问 frame[t+1:]
   - 方法二：遮挡测试，遮挡未来帧不应改变当前预测
   - 方法三：梯度流分析，∂yₜ/∂xₛ (s>t) 应为结构零

2. **状态重置**：
   - 每个视频开始前重置模型状态（清空缓冲区、隐状态）
   - 模拟真实部署（每个视频流独立处理）

3. **批大小限制**：
   - 必须使用 batch=1（逐帧处理）
   - 批量推理会掩盖真实延迟

4. **硬件与环境**：
   - 报告 GPU 型号、显存、驱动版本
   - 报告精度（FP32/FP16/INT8）
   - 报告框架（PyTorch/TensorFlow/ONNX）
   - 报告并发设置（单线程/多线程）

### 6.2.4 离线与在线评测对比

表 6.2 对比了离线与在线评测的主要差异。

**关键区别**：

**（1）数据访问模式**：
- 离线：批量加载，可多次前向传播（如集成学习）
- 在线：流式输入，仅一次前向传播

**（2）评测维度**：
- 离线：单一维度（准确率）
- 在线：多维度（准确率、延迟、吞吐量、鲁棒性）

**（3）优化目标**：
- 离线：准确率最大化
- 在线：Pareto 前沿（准确率-延迟权衡）

## 6.3 在线评测的特殊要求

### 6.3.1 硬件归一化与公平对比

由于在线推理对硬件敏感，需要建立归一化的评测标准。

**推荐做法**：

**（1）基准硬件**：
- GPU：NVIDIA V100 或 A100（学术界常用）
- CPU：Intel Xeon（边缘设备测试）
- 边缘设备：Jetson AGX Xavier（嵌入式部署）

**（2）归一化指标**：

**相对加速比（Relative Speedup）**：
```
Speedup = Latency_baseline / Latency_method
```

以某个基线方法（如因果 Transformer 基础版）为参照，报告相对加速比。

**FLOPs 归一化延迟**：
```
Latency_normalized = Latency_measured * (FLOPs_baseline / FLOPs_measured)
```

消除模型大小差异的影响，仅比较算法效率。

**（3）延迟分解报告**：

| 模块 | 延迟 (ms) | 占比 (%) | 优化空间 | 计时方法 |
|------|----------|---------|---------|---------|
| 数据预处理 | 2.1 | 14% | 低 | CPU 高精度时钟 |
| 视觉编码 | 5.3 | 35% | 中（可换轻量骨干） | CUDA events |
| 音频编码 | 1.8 | 12% | 低 | CUDA events |
| 时序建模 | 4.5 | 30% | 高（核心瓶颈） | CUDA events |
| 输出头 | 1.3 | 9% | 低 | CUDA events |
| **总计** | **15.0** | **100%** | - | - |

*注：使用 CUDA events 或 CPU 高精度时钟（如 `time.perf_counter()`）；延迟为示例值，实际需根据具体硬件测量*

这种分解有助于识别瓶颈和优化方向。

### 6.3.2 延迟容忍度的讨论

不同应用场景对延迟的容忍度差异巨大，需要根据应用需求选择合适的 Δt。

**应用场景-延迟容忍度映射**：

| 应用场景 | 推荐 Δt | 理由 |
|---------|---------|------|
| **L4 自动驾驶** | 0-20ms | 安全关键，延迟直接影响刹车距离 |
| **工业质检** | 0-50ms | 传送带速度快，需要即时剔除 |
| **智能监控（异常检测）** | 50-200ms | 异常响应可容忍小延迟 |
| **直播内容审核** | 500-1000ms | 观众体验可接受轻微延迟 |
| **会议记录/字幕** | 1000-2000ms | 可接受较大延迟 |
| **视频归档分析** | 无限制 | 非实时，可用离线方法 |

**延迟-性能曲线分析**：

图 6.1 展示了典型在线模型的延迟容忍度-性能曲线。从图中可以看出：

1. **陡峭区（Δt = 0-100ms）**：性能快速提升，斜率大
2. **平缓区（Δt = 100-500ms）**：性能提升放缓，接近离线上限
3. **饱和区（Δt > 500ms）**：性能基本不再提升，已接近离线

**建议评测点**：Δt = {0, 50, 100, 200, 500, 1000} ms，覆盖不同应用场景。

### 6.3.3 多维度综合评估

在线方法需要在**准确率、延迟、内存、能耗**等多个维度间权衡，单一指标无法全面评价。

**Pareto 前沿分析**：

```
准确率 (%)
   ↑
 78|     ● 离线 SOTA（MMIL）
   |       
 76|   ● 离线（AVVP）
   |
 74|       ● StreamAV（在线）
   |     ● PreFM（在线）
 72|   
   | ● 轻量在线（GRU）
 70|
   |________________→ 延迟 (ms)
    0   10  20  30  40
```

**Pareto 最优**：StreamAV 和 PreFM 在各自的延迟范围内达到最优性能，构成 Pareto 前沿。

**综合评分（Composite Score）**：

某些工作提出综合评分公式：
```
Score = α·F1 + β·(1/Latency) + γ·(1/Memory)
```

其中 α, β, γ 为权重系数（根据应用场景设定）。

**示例**（α=0.6, β=0.3, γ=0.1）：
| 模型 | F1 (%) | 延迟 (ms) | 显存 (GB) | Score |
|------|--------|----------|----------|-------|
| AVVP（离线） | 75 | 8500 | 3.2 | 0.45 + 0.00035 + 0.031 = 0.48 |
| PreFM（在线） | 73 | 15 | 2.1 | 0.438 + 0.020 + 0.048 = 0.51 |
| StreamAV（在线） | 73 | 12 | 2.3 | 0.438 + 0.025 + 0.043 = 0.51 |

在线方法虽然准确率略低，但综合得分更高。

**能耗与效率**（可选指标，适用于边缘部署场景）：

- **Energy/frame (J)**：单帧处理的能量消耗（焦耳）
  ```
  Energy/frame = Power_avg × Latency_avg
  ```

- **W@RTF=1**：实时运行（RTF=1）时的平均功耗（瓦特）
  ```
  测量方法：使用 nvidia-smi --query-gpu=power.draw 
           每 100ms 采样，持续 60s，取平均
  ```

**典型能耗对比**（示例，基于 V100 GPU）：
| 模型 | 单帧延迟 (ms) | 单帧能耗 (J) | W@RTF=1 | 适合场景 |
|------|--------------|-------------|---------|---------|
| PreFM | 15 | 4.5 (300W×0.015s) | ~300W | 数据中心 |
| StreamAV（无优化） | 12 | 3.6 | ~300W | 数据中心 |
| StreamAV（INT8） | 6 | 1.2 | ~200W | 边缘设备 |

*注：边缘设备（Jetson Xavier）功耗预算通常 10-30W，需要量化和剪枝优化*

## 6.4 数据集的挑战与局限

### 6.4.1 现有数据集的局限性

**（1）缺乏在线特有场景**：
- 现有数据集（AVE、LLP）主要为离线评测设计
- 缺少模态异步、缺失、噪声等在线特有挑战
- 音视频同步良好，未包含时钟漂移、抖动等真实问题

**（2）规模有限**：
- AVE：4,143 视频（相比 ImageNet 的 120 万图像，规模小 **300 倍**）
- LLP：约 11,000 视频（仍远小于大规模数据集）
- 限制了大模型预训练和数据驱动方法的发展

**（3）标注粗糙**：
- LLP 仅提供视频级标签（弱监督），缺少精确时间边界
- UnAV-100 未配对，不适合直接的事件解析评测
- 缺少模态可见性的细粒度标注（如部分可见、遮挡等）

**（4）类别覆盖有限**：
- AVE 28 类，LLP 约 25 类，覆盖的事件类型有限
- 缺少复杂场景（多事件重叠、长视频、多说话人等）
- 缺少开放词汇（open-vocabulary）评测

### 6.4.2 未来数据集需求

为更好地支持 OMU 研究，未来数据集应包含：

**（1）在线特有挑战**：
- **模态异步数据**：故意引入音视频时间偏移（±50-500ms）
- **模态缺失数据**：随机丢失某些帧的视觉或音频信息
- **噪声与干扰**：背景噪声、视觉遮挡、光照变化等

**（2）更大规模**：
- 视频数：10 万+（可支持大模型预训练）
- 类别数：100-500 类（更细粒度、更广覆盖）
- 长视频：包含分钟级甚至小时级视频（测试长期记忆）

**（3）细粒度标注**：
- 帧级标注（而非片段级）
- 模态可见性的连续值（0-1，而非离散类别）
- 多事件重叠标注
- 事件边界的软标注（考虑标注不确定性）

**（4）多模态扩展**：
- 三模态或更多（视觉、音频、文本、深度、IMU 等）
- 跨语言、跨文化场景
- 合成数据与真实数据混合

**推荐扰动强度区间**（用于构建在线挑战数据集）：
- **模态异步**：±{50, 100, 200, 500} ms
- **模态缺失**：{10%, 30%, 50%} 随机帧
- **时基抖动**：±{10, 20, 50} ms

## 6.5 本章小结

本章系统介绍了 OMU 任务常用的数据集和评测指标，主要结论如下：

**（1）数据集概览**：
- **AVE**：4,143 视频，28 类，片段级标注，适合快速评测
- **LLP**：约 11,000 视频，约 25 类，弱监督+模态可见性，更具挑战
- **UnAV-100**：33,924 片段，100 类，未配对设计，适合预训练和对齐学习

**（2）评测指标体系**：
- **离线指标**：Segment F1（AVE）、Event-wise mAP（LLP）、Frame Accuracy
- **在线指标**：Acc@Δt（延迟容忍）、RTF（实时因子）、Missing-Rate/Audio-Delay/Jitter-Tolerance（鲁棒性）

**（3）在线评测特殊要求**：
- 多维度评估（准确率+延迟+内存+鲁棒性）
- 延迟容忍度曲线（多个 Δt 设定）
- 硬件归一化与公平对比
- 因果性验证与状态重置

**（4）数据集局限与未来需求**：
- 现有数据集缺乏在线特有挑战（模态异步、缺失、抖动）
- 规模有限（制约大模型发展）
- 标注粗糙（弱监督、缺少细粒度）
- 未来需要更大规模、更细粒度、包含在线挑战的数据集

**（5）评测建议**：
- **报告完整曲线**：延迟容忍度曲线、鲁棒性曲线（而非单点）
- **分解延迟**：报告各模块延迟占比，识别瓶颈
- **硬件条件**：明确注明 GPU/CPU、精度、分辨率等
- **Pareto 分析**：在准确率-延迟空间可视化，而非单一综合分数

在下一章中，我们将讨论 OMU 的应用场景与面临的主要挑战，分析如何在实际部署中平衡延迟与精度、处理模态异步等工程问题。

---

## 表 6.1 主要数据集对比总结

| 数据集 | 提出者/年份 | 规模 | 类别 | 标注方式 | 模态可见性 | 主要任务 | 在线适用性 |
|--------|-----------|------|------|---------|----------|---------|----------|
| **AVE** | Tian et al., 2018 | 4,143 视频 | 28 | 片段级（1s） | ✗ | AVEL | 高 |
| **LLP** | Tian et al., 2020 | ≈11,000 视频 | ≈25 | 弱监督（视频级） | ✓ | AVEP/AVVP | 高 |
| **UnAV-100** | Lin et al., 2021 | 33,924 片段 | 100 | 未配对 | ✗（未提供） | 对齐学习 | 中（预训练） |
| **ActivityNet** | Heilbron et al., 2015 | 20,000 视频 | 200 | 时序段级 | ✗ | 动作定位 | 中（长视频） |
| **Kinetics-400** | Kay et al., 2017 | 30 万片段 | 400 | 片段级（10s） | ✗ | 动作识别 | 中（预训练） |
| **AudioSet** | Gemmeke et al., 2017 | 200 万片段 | 632 | 片段级（10s） | - | 音频事件 | 低（纯音频） |

**适用性说明**：
- **高**：直接适合在线评测，有片段级标注
- **中**：需要适配或主要用于预训练
- **低**：不适合在线多模态评测

*注：类别数与规模为文献常见报告值；LLP 类别数在不同实现中可能为 23-25 类*

**数据集许可与可复现性**：
- **AVE/LLP**：基于 YouTube 视频构建，原始链接有效性随时间变化（部分视频可能被删除）；建议使用官方发布的特征文件或镜像数据集
- **UnAV-100**：官方提供下载链接，数据相对稳定
- **Kinetics/AudioSet**：官方提供视频 ID 列表，需自行下载（部分链接失效率 10-30%）
- **复现建议**：记录使用的数据集版本、下载日期、预处理脚本版本（如官方 repo 的 commit hash），确保可复现

---

## 表 6.2 离线与在线评测协议对比

| 维度 | 离线评测 | 在线评测 |
|------|---------|---------|
| **数据访问** | 批量加载完整视频 | 流式逐帧输入 |
| **推理模式** | 一次性批量推理（batch 8-32） | 逐帧推理（batch=1） |
| **状态管理** | 可重复前向（集成学习） | 仅一次前向，需状态重置 |
| **因果性要求** | 无要求（可用未来信息） | 严格因果约束 |
| **主要指标** | F1、mAP（准确率为主） | F1 + 延迟 + 吞吐量 + 鲁棒性 |
| **延迟评估** | 不评估或仅参考 | 核心指标（avg/max/p95） |
| **鲁棒性评估** | 通常不评估 | Missing-Rate、Audio-Delay、Jitter |
| **硬件敏感性** | 低（主要看 FLOPs） | 高（实际延迟受硬件影响大） |
| **优化目标** | 准确率最大化 | Pareto 前沿（多目标权衡） |
| **后处理** | 允许（如 CRF、平滑） | 必须因果（不可用全局平滑） |
| **评测复杂度** | 低（单次运行） | 高（需多次运行，统计延迟分布） |

---

## 图 6.1 延迟容忍度-性能曲线（示意图）

```
Segment F1 (%)
   ↑
 75|                    ────────────  离线上限
   |                ●──────●
 74|             ●─●           ● PreFM
   |          ●─●            ●  StreamAV
 73|       ●─●
   |    ●─●
 72| ●─●
   |●                          陡峭区 → 平缓区 → 饱和区
 71|
   |___________________________________________→ Δt (ms)
    0    50   100  200      500     1000

说明：
- Δt=0: 严格实时（72-73%）
- Δt=100: 人眼难察觉（73-74%，+1-2%）
- Δt=500: 接近离线（74-75%，+2-3%）
- Δt>1000: 饱和，性能不再提升

实验条件：曲线为 F1@Δt（Strict 模式），AVE 数据集，
         因果 Transformer 架构，batch=1，224×224，
         FP16，V100 GPU，阈值固定 0.5
```

---

## 表 6.3 评测指标完整对照表

| 指标类别 | 指标名称 | 计算方式 | 适用数据集 | 适用模式 | 说明 |
|---------|---------|---------|-----------|---------|------|
| **准确率** | Segment F1 | 片段级 Precision/Recall | AVE | 离线+在线 | 默认微平均 |
| | Event-wise mAP | 类别级 AP 均值 | LLP | 离线+在线 | 适合多标签 |
| | Frame Accuracy | 逐帧准确率 | 通用 | 离线+在线 | 对边界敏感 |
| **延迟** | Latency_avg | 平均单帧处理时间 | 通用 | 仅在线 | 核心指标 |
| | Latency_p95 | 95 分位延迟 | 通用 | 仅在线 | 反映稳定性 |
| | Latency_max | 最大延迟 | 通用 | 仅在线 | 最坏情况 |
| **吞吐** | FPS | 1 / Latency_avg | 通用 | 仅在线 | 处理能力 |
| | RTF | 处理时长/视频时长 | 通用 | 仅在线 | 实时性判定 |
| **延迟容忍** | Acc@Δt | Δt 内正确预测比例 | 通用 | 仅在线 | 应用场景相关 |
| **鲁棒性** | Missing-Rate@p% | p% 缺失下的 F1 | 通用 | 仅在线 | 模态缺失容忍 |
| | Audio-Delay@Δt | Δt 音频延迟下的 F1 | 通用 | 仅在线 | 异步容忍 |
| | Jitter-Tolerance@±δ | ±δ 抖动下的 F1 | 通用 | 仅在线 | 时基稳定性 |
| **资源** | 参数量 | 模型参数总数 | 通用 | 离线+在线 | 存储需求 |
| | FLOPs | 每帧浮点运算数 | 通用 | 离线+在线 | 计算复杂度 |
| | 显存占用 | GPU 内存峰值 | 通用 | 离线+在线 | 部署约束 |
| **能耗** | Energy/frame | 单帧能量消耗 (J) | 通用 | 仅在线 | 边缘部署相关 |
| | W@RTF=1 | 实时运行功耗 (W) | 通用 | 仅在线 | 功耗预算 |
| **不确定性** | ECE | 校准误差 | 通用 | 离线+在线 | 置信度质量 |
| | Brier Score | 概率预测MSE | 通用 | 离线+在线 | 概率准确性 |

**使用建议**：
- **离线方法**：主要报告准确率（F1/mAP）+ 参数量 + FLOPs
- **在线方法**：必须报告准确率 + 延迟（avg/p95/max）+ 吞吐量 + Acc@Δt 曲线 + 显存
- **鲁棒性评估**：可选但推荐（Missing-Rate、Audio-Delay、Jitter）
- **边缘部署**：推荐报告能耗指标（Energy/frame、W@RTF=1）
- **置信度应用**：若涉及报警/联动，建议报告 ECE 或 Brier Score

---

## 本章参考文献

[1] Tian, Y., Shi, J., Li, B., Duan, Z., and Xu, C. Audio-visual event localization in unconstrained videos. In Proceedings of the European Conference on Computer Vision (ECCV), pages 247-263, 2018.

[2] Tian, Y., Li, D., and Xu, C. Unified multisensory perception: Weakly-supervised audio-visual video parsing. In Proceedings of the European Conference on Computer Vision (ECCV), pages 436-454, 2020.

[3] Lin, Y., Wang, X., Zhang, B., Zhao, H., and Li, J. UnAV-100: A dataset for unsupervised audio-visual learning. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 9821-9830, 2021.

[4] Krishna, R., Hata, K., Ren, F., et al. Dense-captioning events in videos. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 706-715, 2017.

[5] Kay, W., Carreira, J., Simonyan, K., et al. The Kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.

[6] Gemmeke, J.F., Ellis, D.P., Freedman, D., et al. Audio Set: An ontology and human-labeled dataset for audio events. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 776-780, 2017.

