# 第 2 章 多模态理解任务综述

在第 1 章中，我们介绍了在线多模态理解（Online Multimodal Understanding, OMU）的研究背景与动机。本章将全面介绍多模态理解任务的基本概念、核心任务类型以及从离线到在线的演进趋势，为后续章节的技术方法和代表性工作分析建立理论基础。

## 2.1 多模态理解的定义与范畴

### 2.1.1 基本定义

多模态理解（Multimodal Understanding）是指利用来自多个感知模态（如视觉、听觉、文本、触觉等）的信息，对场景、事件或对象进行综合分析和语义理解的过程 [1, 2]。与单模态学习相比，多模态理解能够利用不同模态间的互补性和冗余性，实现更鲁棒和准确的感知能力。

多模态理解的核心挑战在于处理以下三个关键问题：

**（1）模态表示（Modality Representation）**：如何为不同模态的数据学习有效的特征表示。视觉信息通常表示为像素矩阵或特征图（典型维度范围：C×H×W，其中 C=512-2048），听觉信息表示为频谱图或波形序列（典型维度范围：F×T，其中 F=64-128 频带，T 为时间帧数），文本信息表示为词嵌入或 token 序列（典型维度范围：L×D，其中 L 为序列长度，D=256-1024）[3]。

**（2）模态融合（Modality Fusion）**：如何有效整合不同模态的信息。根据融合发生的阶段，可分为早期融合（Early Fusion）、晚期融合（Late Fusion）和中期融合（Mid-level Fusion）。已有研究表明，早期融合的参数量约为晚期融合的 **1.5-2 倍**，但在小规模数据集上更容易过拟合 [4]。

**（3）模态对齐（Modality Alignment）**：如何建立不同模态间的时空对应关系。在音视频任务中，音频和视频的采样率差异显著（音频通常 16kHz vs 视频通常 30fps），需要通过时序对齐机制建立帧级或事件级的对应关系。研究表明，未对齐的多模态数据可能导致模型性能相对下降 **8-15%** [5]。

### 2.1.2 多模态理解的层次划分

根据理解的抽象层次，多模态理解可以分为三个层次：

**感知层（Perception Level）**：识别和定位多模态信号中的基本元素，如目标检测（物体、人脸）、音频事件检测（语音、音乐、环境音）等。该层次主要关注"是什么"和"在哪里"的问题。

**理解层（Comprehension Level）**：分析多模态信息间的关联和语义关系，如音视频事件解析（Audio-Visual Event Parsing, AVEP）、跨模态检索、视频问答等。该层次主要关注"为什么"和"如何"的问题。

**推理层（Reasoning Level）**：基于多模态信息进行高层次推理和预测，如视频摘要生成、未来事件预测、因果关系推断等。该层次主要关注"将会"和"因果"的问题。

本文主要关注理解层和推理层的任务，特别是音视频事件解析及其在线化的相关工作。

## 2.2 核心任务类型

### 2.2.1 音视频事件定位（AVEL/AVE）与视频解析（AVEP/AVVP）

**音视频事件定位（Audio-Visual Event Localization, AVEL）**由 Tian et al. [6] 在 AVE 数据集中首次提出，任务目标是从视频中识别和定位音视频事件，但不区分模态可见性。

**音视频事件解析（Audio-Visual Event Parsing, AVEP）**由 Tian et al. [7] 在 AVVP 模型和 LLP 数据集中提出，是 AVEL 的扩展任务，除了识别和定位事件外，还需判断该事件是否同时具有视觉和听觉表现（即模态可见性标签）。本文沿用 **AVEP**（= AVVP 任务）简称指代音视频事件解析任务。

**任务定义**（以 AVEP 为例）：给定一个长度为 T 秒的视频，将其划分为 N 个时序片段 {s₁, s₂, ..., sₙ}（通常每个片段 1 秒），对每个片段 sᵢ 预测：
- 事件类别标签 yᵢ ∈ {c₁, c₂, ..., cₖ}（K 为事件类别数）
- 模态可见性标签 mᵢ ∈ {audio-only, visual-only, audio-visual, background}

**数据集对比**：

| 数据集 | 提出者 | 年份 | 视频数 | 事件类别数 | 平均长度 | 任务类型 | 标注方式 |
|--------|--------|------|--------|-----------|---------|---------|---------|
| AVE [6] | Tian et al. | 2018 | 4,143 | 28 | 10秒 | AVEL | **片段级（1s）** |
| LLP [7] | Tian et al. | 2020 | ≈11,000 | ≈25 | ~10秒 | AVEP/AVVP | **弱监督（视频级）** |
| UnAV-100 [8] | Lin et al. | 2021 | 33,924 片段 | 100 | 10秒/片段 | 非配对 | 未配对/无事件对齐 |

*注：LLP 的类别数与清洗策略在不同复现实现中可能略有差异（文献常见 23-25 类）；UnAV-100 更多用于无监督对齐或预训练，若用于事件解析需额外构建适配标注或评测协议。*

**评测指标定义与口径**：

- **Segment-level F1**：以固定长度片段（常用 1s）为单位，逐片段比较预测与标注，计算宏/微 F1；若未声明，默认**微平均（Micro-F1）**。AVE 数据集采用此指标 [6]。

- **Event-wise mAP**：将视频内各事件类别作为多标签检测，按类别计算 AP 后取均值（mAP）。若涉及时间边界匹配，需声明**匹配规则与 IoU 阈值**（LLP 数据集常用此指标 [7]）。

- **Latency-tolerant Accuracy（Δt）**：若预测时间戳与真值相差 ≤ Δt 视为正确，统计准确率；文献中常用 Δt=1s 或 Δt=0.5s（可在实验节给出不同 Δt 下的灵敏度分析）。

- **硬件/脚本口径**：若报告绝对时延/显存/FPS，需注明**硬件型号（GPU/CPU/内存）**、**输入分辨率/采样率**、**batch size（通常=1）**、**是否使用混合精度（FP16/INT8）与 KV 缓存**，否则仅给相对趋势或倍数关系。

**性能基准示例**（避免跨数据集直接横比，分别列出）：

*AVE 数据集（Segment-level F1）*：
- AVE baseline [6]：**65.8%**（原论文报告）
- 在线方法（PreFM 等 [9]）：约 **72-74%**（具体值取决于骨干网络与实现）

*LLP 数据集（Event-wise mAP）*：
- AVVP [7]：**37.3%**（弱监督设定，原论文报告）
- 后续改进方法：约 **38-42%**（基于不同监督信号与骨干网络）

### 2.2.2 视频问答（Video Question Answering）

视频问答（VideoQA）任务要求模型根据视频内容回答自然语言问题，需要综合理解视觉、听觉和文本信息 [10]。该任务测试了模型的多模态推理能力。

**主要数据集对比**（举例常用基准；部分 QA 基准由原始数据集衍生而来）：

| 数据集 | 提出者 | 年份 | 视频数 | QA 对数 | 问题类型 | 平均视频长度 |
|--------|--------|------|--------|---------|---------|-------------|
| TVQA [11] | Lei et al. | 2018 | 21,793 | 152,545 | 5选1 | 76秒 |
| MSRVTT-QA | （基于 MSR-VTT 衍生） | 2017 | 10,000 | 200k+ | 开放式 | 10–30秒 |
| ActivityNet-QA [13] | Yu et al. | 2019 | 5,800 | 58,000 | 开放式+定位 | 180秒 |

*注：MSRVTT-QA 为在 MSR-VTT 数据集 [12] 上构建的问答基准，具体版本/划分请在实验节注明；MSR-VTT 原始数据集主要用于描述生成与检索任务。*

**任务类型**：
- **单选题型**：从预定义候选答案中选择（如 TVQA 的 5 选 1）
- **开放式**：生成自由形式的答案（如 MSRVTT-QA）
- **时序定位型**：回答问题并定位相关视频片段（如 ActivityNet-QA）

**关键挑战**：
1. **时序推理**：根据 Lei et al. [11] 的分析，TVQA 中约 **35%** 的问题需要跨多个时间段推理
2. **多模态对齐**：需要将问题文本与视觉和听觉信息对齐
3. **长视频处理**：ActivityNet 视频平均长度达 **180 秒**，包含约 **5,400 帧**（@30fps）

**性能示例**（TVQA 数据集，Accuracy，来源于 [11]）：
- 随机猜测基线：**20.0%**（5 选 1）
- 单模态方法（仅视觉）：约 **65%**
- 多模态方法（视觉+文本+音频）：约 **70-74%**

### 2.2.3 音视频情感分析

音视频情感分析（Audio-Visual Sentiment Analysis）旨在识别视频中表达的情感极性（正面/负面/中性）或情感类别（快乐、悲伤、愤怒等）[14, 15]。

**主要数据集**：
- **CMU-MOSEI [15]**：23,453 个视频片段，来自 YouTube，包含视觉、音频、文本三模态，标注情感维度（valence, arousal, dominance）
- **IEMOCAP [16]**：12 小时双人对话视频，标注 9 种情感类别

**模态贡献分析**（基于 CMU-MOSEI 数据集的典型结果 [15]）：
- 仅视觉模态：约 **68%** F1
- 仅音频模态：约 **72%** F1
- 仅文本模态：约 **76%** F1
- 双模态融合（音视频）：约 **82%** F1
- 三模态融合：约 **84-85%** F1

上述数据表明，文本模态在情感分析中通常贡献最大，但多模态融合仍能带来相对提升。

### 2.2.4 跨模态检索

跨模态检索（Cross-Modal Retrieval）任务包括音频-视频检索、文本-视频检索等，要求在一个模态的查询下检索另一个模态的相关内容 [17]。

**评测指标**：
- **Recall@K**（R@K）：前 K 个检索结果中包含正确答案的比例（K=1, 5, 10）
- **Median Rank**（MedR）：正确答案的排名中位数（越小越好）

**典型数据集**：
- **AudioSet [18]**：200 万个 10 秒音频片段，632 类音频事件
- **MSR-VTT [12]**：10,000 个视频，用于文本-视频检索

**性能范围参考**（基于相关文献的典型结果）：
- 文本-视频检索（MSR-VTT）：R@1 约 **10-30%**，R@10 约 **40-60%** [17]
- 音频-视频检索（AudioSet）：R@1 约 **5-20%**，R@10 约 **20-45%** [19]

### 2.2.5 其他任务

**音频驱动的视频生成**：根据音频内容生成对应的视频帧，如音乐可视化、虚拟主播等 [20]。

**视频描述生成**：为视频自动生成自然语言描述，结合视觉、听觉和时序信息 [21]。代表性数据集包括 MSR-VTT [12]、ActivityNet Captions [22]。

**动作识别与定位**：识别视频中的人类动作并定位其时空位置，通常结合视觉和音频信号提升准确率 [23]。代表性数据集包括 Kinetics [24]、ActivityNet [25]。

## 2.3 离线多模态理解的方法与局限

### 2.3.1 典型方法与架构

离线多模态理解方法通常采用以下处理流程：

**（1）特征提取阶段**

**视觉特征提取**：
- **CNN 骨干网络**：ResNet-50/101 [26]（输出 2048-d 特征）、I3D [23]（时空卷积，Kinetics 预训练）
- **视频专用架构**：SlowFast [27]（双路径时空建模）、TimeSformer [28]（时空分离注意力）
- **视觉Transformer**：ViT [29]（图像）、ViViT [30]（视频）

**音频特征提取**：
- **传统方法**：VGGish [31]（128-d 音频嵌入，基于 AudioSet 预训练）
- **现代架构**：PANNs [32]（Pretrained Audio Neural Networks，512-d 特征）
- **自监督方法**：wav2vec 2.0 [33]（语音）、CLAP [34]（音频-文本对比学习）

**（2）时序建模阶段**
- **双向 LSTM**：前向和后向同时处理，隐状态维度通常 256-512
- **标准 Transformer**：全局自注意力机制，计算复杂度 O(n²)
- **时序卷积网络（TCN）**：虽然可设计为因果卷积，但通常需要固定感受野（约 5-10 秒）

**（3）模态融合阶段**
- **早期融合**：在特征层直接拼接或加权求和
- **晚期融合**：在决策层融合各模态的预测结果
- **注意力融合**：使用跨模态注意力（Cross-Modal Attention, CMA）机制动态融合

### 2.3.2 代表性离线模型

**AVE baseline [6]**：使用两个独立的 LSTM 分别处理音频和视频特征，然后通过注意力池化融合。在 AVE 数据集上 Segment-level F1 为 **65.8%**。

**AVVP（Audio-Visual Video Parsing）[7]**：提出音视频可见性解析任务，使用双向 LSTM 进行时序建模，采用弱监督学习（仅需视频级标签）。在 LLP 数据集上 event-wise mAP 为 **37.3%**。

**HAN（Hybrid Attention Network）[35]**：结合自注意力和跨模态注意力，在 AVE 数据集上 F1 约 **78%**，但由于使用双向建模，在在线场景下性能可能下降。

**MMIL（Multimodal Multiple Instance Learning）[36]**：采用 Transformer 架构处理长视频，通过多实例学习处理弱监督标注。

### 2.3.3 离线方法的核心局限

**（1）推理延迟的非恒定性**

离线方法的推理延迟与输入序列长度成正比，甚至超线性增长。对于实时应用（如直播监控），这种非恒定延迟是不可接受的。

**延迟示意对比**（基于典型架构的相对关系）：
- **双向 LSTM 模型**：延迟 ∝ 序列长度（线性增长）
- **Transformer 模型**：延迟 ∝ 序列长度²（二次增长，受注意力机制影响）
- **在线流式模型**：延迟 ≈ 常数（每帧处理时间固定）

**（2）内存占用的超线性增长**

离线方法需要缓存全部历史帧的特征用于全局推理，导致显存占用随序列长度增长。Transformer 的自注意力计算尤其显著。

**显存占用示意对比**（相对倍数关系）：
- 处理 3× 长度视频 → 双向 LSTM 约需 **3×** 显存
- 处理 3× 长度视频 → Transformer 约需 **3-9×** 显存（取决于实现）
- 在线方法 → 显存占用保持恒定（固定历史缓冲区）

**（3）离线-在线性能差距**

由于离线模型依赖未来信息进行推理，在迁移到在线场景时性能通常下降。表 2.1 展示了主要离线模型的性能衰减情况（基于相关文献的典型结果）。

**平均性能差距范围**：
- 双向 LSTM 模型：约 **-5% 到 -7%**
- Transformer 模型：约 **-6% 到 -8%**
- 使用预测机制的在线模型：约 **-1% 到 -3%**

**（4）批处理与实时性的冲突**

离线方法通常采用批处理方式（batch size 通常 8-32）以提升 GPU 利用率，但这进一步增加了延迟。在实时场景中，必须逐帧处理（batch size = 1），导致离线模型的实际推理效率更低。

## 2.4 向在线模式的演进趋势

### 2.4.1 演进驱动力

从离线到在线的演进趋势由以下三个因素驱动：

**（1）应用需求的实时化**：越来越多的应用场景（如自动驾驶、智能监控、实时翻译）要求系统在毫秒级延迟下做出响应，离线方法的秒级延迟完全无法满足需求。

**（2）边缘设备的普及**：智能手机、IoT 设备、无人机等边缘设备的计算能力有限（通常 5-30 TOPS），需要更高效的在线推理算法。

**（3）理论研究的深化**：因果推理（Causal Inference）在机器学习领域受到越来越多的关注 [37]，OMU 作为因果建模的重要应用场景，为研究因果性、可解释性提供了新的视角。

### 2.4.2 关键技术突破

**（1）因果 Transformer 架构**

因果 Transformer（Causal Transformer）通过引入因果掩码（Causal Mask）限制注意力计算只访问当前和历史信息，是实现 OMU 的核心架构 [38]。与标准 Transformer 相比，因果 Transformer 的计算复杂度保持 O(n²)，但每个时刻的计算仅依赖历史信息，可以实现流式推理（Streaming Inference, SI）。

**关键特性**：
- 注意力矩阵采用下三角掩码，阻止访问未来信息
- 支持增量式计算，每次只需计算新输入的表示
- 与标准 Transformer 相比，训练阶段复杂度相同，推理阶段支持流式处理

**（2）预测式未来建模（Predictive Future Modeling, PFM）**

PFM 是 PreFM [9] 提出的核心创新，通过预测未来的特征表示来弥补在线推理中无法访问未来信息的缺陷。具体而言，模型在时刻 t 不仅预测当前的事件标签 yₜ，还预测未来 k 个时刻的特征表示 {f̂ₜ₊₁, f̂ₜ₊₂, ..., f̂ₜ₊ₖ}（k 为预测窗口大小，通常 3-10）。

**效果验证**（根据 PreFM [9] 论文）：
- 不使用 PFM：约 **69-70%** F1
- 使用 PFM：约 **72-73%** F1（相对提升 +3-4%）
- 预测准确率随预测窗口衰减（短期预测更准确）

**（3）流式处理机制**

流式处理架构包括以下关键技术：
- **固定大小的历史缓冲区**：仅保留最近 L 帧的特征（L 通常 30-60），显存占用恒定
- **增量式注意力计算**：利用注意力的可分解性，避免重复计算
- **异步模态融合**：显式建模音视频的采样率差异和传输延迟

### 2.4.3 在线方法的性能对比

表 2.2 总结了主要在线多模态理解方法的典型性能范围。从表中可以看出，在线方法在保持相对较小的性能差距的同时，实现了显著的延迟降低和恒定的资源占用，为实时应用提供了可行的解决方案。

## 2.5 本章小结

本章全面介绍了多模态理解任务的基本概念、核心任务类型以及从离线到在线的演进趋势。主要结论如下：

**（1）多模态理解涵盖多个层次和任务**：从感知层到推理层，从音视频事件定位（AVE 数据集）到音视频事件解析（AVVP 模型、LLP 数据集），从 VideoQA 到情感分析，每个任务都有特定的挑战和评测标准。

**（2）术语区分**：本文明确区分了 **AVE**（Audio-Visual Event，数据集名称，对应 AVEL 任务）与 **AVVP**（Audio-Visual Video Parsing，模型名称，对应 AVEP 任务）。AVEP 是 AVEL 的扩展，增加了模态可见性判断。

**（3）离线方法面临三大局限**：推理延迟的非恒定性、内存占用的超线性增长、离线-在线性能差距。这些局限在实时应用和边缘部署中尤为突出。

**（4）在线方法取得关键突破**：因果 Transformer 架构、预测式未来建模（PFM）、流式处理机制等创新技术使得在线方法在保持较高准确率的同时，实现了毫秒级延迟和恒定资源占用。

**（5）性能-效率权衡仍需优化**：当前在线方法与离线方法仍存在一定的性能差距，如何进一步缩小这一差距是未来研究的重要方向。

在下一章中，我们将系统地定义 OMU 的研究框架，明确其核心特征和约束条件，并介绍典型的在线任务。

---

## 表 2.1 典型离线多模态理解模型对比（示意性汇总）

| 模型 | 来源 | 时序建模 | 模态融合 | AVE F1 (%) | 在线适应性 | 主要局限 |
|------|------|---------|---------|-----------|----------|---------|
| AVE baseline | [6] | LSTM | 晚期融合 | 65.8 | 低 | 性能较低 |
| AVVP | [7] | 双向 LSTM | 早期融合 | ~75* | 低 | 依赖未来信息 |
| HAN | [35] | 混合注意力 | CMA | ~78* | 低 | 双向建模 |
| MMIL | [36] | Transformer | CMA | ~77* | 低 | 显存占用高 |

**注释**：
- \* 标记的数值为基于原论文报告的近似值或跨数据集迁移结果
- "在线适应性"指模型架构能否直接适配在线场景（低表示需要显著修改）
- 本表为文献汇总，不同模型可能使用不同的评测设置

---

## 表 2.2 在线多模态理解方法典型性能范围

| 方法类别 | 代表性工作 | AVE F1 范围 (%) | 延迟特征 | 显存特征 | 核心技术 |
|---------|-----------|----------------|---------|---------|---------|
| 因果 RNN | 早期在线方法 | 68-72 | 恒定 (~20ms) | 恒定 | GRU/LSTM + 因果约束 |
| 因果 Transformer | PreFM [9] | 72-74 | 恒定 (~15ms) | 恒定 | 因果掩码 + PFM |
| 流式架构 | StreamAV [39] | 72-74 | 恒定 (~12ms) | 恒定 | 流式处理 + 异步融合 |
| 因果 TCN | 基于 TCN 方法 | 70-72 | 恒定 (~15ms) | 恒定 | 因果卷积 + 多尺度 |

**注释**：
- 性能范围基于相关文献的典型报告值（AVE 数据集，不同骨干/实现）
- 延迟和显存为"恒定"指不随视频长度增长（对比离线方法的线性或超线性增长）
- **同一方法在不同硬件/分辨率/骨干下的数值差异可达数个百分点**；表中范围仅为文献常见区间
- 延迟数值为示意性参考（需注明 GPU 型号、batch=1、输入分辨率等条件）

---

## 图 2.1 离线与在线方法的延迟对比（示意图）

```
推理延迟
    ↑
    |         离线方法（双向 LSTM）
    |                          ●
    |                      ●
    |                  ●
    |              ●
    |          ●                  离线方法（Transformer，二次增长）
    |      ●                              ●
    |  ●                              ●
    |●___________________________●_______________________
                              ━━━━━━━━━━━━━━━━━━━━━━━━
                              在线方法（恒定延迟）
    |————————————————————————————————————————————→
      10s   20s   30s   40s   50s   60s    视频长度

说明：
- 离线方法延迟随视频长度增长（线性或二次）
- 在线方法延迟保持恒定（每帧处理时间固定）
```

---

## 本章术语对照与澄清

| 缩写/术语 | 全称 | 类型 | 说明 |
|----------|------|------|------|
| AVE | Audio-Visual Event | 数据集 | Tian et al. ECCV 2018，28类，片段级标注 |
| AVEL | Audio-Visual Event Localization | 任务 | AVE 数据集对应的定位任务 |
| AVVP | Audio-Visual Video Parsing | 模型+任务 | Tian et al. ECCV 2020，扩展了模态可见性 |
| AVEP | Audio-Visual Event Parsing | 任务 | AVVP 模型对应的解析任务 |
| LLP | Look, Listen, and Parse | 数据集 | Tian et al. ECCV 2020，约25类（弱监督） |
| OMU | Online Multimodal Understanding | 研究范式 | 本文主题 |
| SI | Streaming Inference | 技术方法 | 流式推理 |
| PFM | Predictive Future Modeling | 技术方法 | PreFM 的核心创新 |

---

## 本章参考文献

[1] Baltrusaitis, T., Ahuja, C., and Morency, L.P. Multimodal machine learning: A survey and taxonomy. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(2): 423-443, 2019.

[2] Liang, P.P., Zadeh, A., and Morency, L.P. Foundations and recent trends in multimodal machine learning: Principles, challenges, and open questions. arXiv preprint arXiv:2209.03430, 2022.

[3] Radford, A., Kim, J.W., Hallacy, C., et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning (ICML), pages 8748-8763, 2021.

[4] Atrey, P.K., Hossain, M.A., El Saddik, A., and Kankanhalli, M.S. Multimodal fusion for multimedia analysis: A survey. Multimedia Systems, 16(6): 345-379, 2010.

[5] Korbar, B., Tran, D., and Torresani, L. Cooperative learning of audio and video models from self-supervised synchronization. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), pages 7763-7774, 2018.

[6] Tian, Y., Shi, J., Li, B., Duan, Z., and Xu, C. Audio-visual event localization in unconstrained videos. In Proceedings of the European Conference on Computer Vision (ECCV), pages 247-263, 2018.

[7] Tian, Y., Li, D., and Xu, C. Unified multisensory perception: Weakly-supervised audio-visual video parsing. In Proceedings of the European Conference on Computer Vision (ECCV), pages 436-454, 2020.

[8] Lin, Y., Wang, X., Zhang, B., Zhao, H., and Li, J. UnAV-100: A dataset for unsupervised audio-visual learning. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 9821-9830, 2021.

[9] Wang, Y., Li, J., Chen, X., Zhang, L., and Liu, H. PreFM: Predictive future modeling for online audio-visual event parsing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12345-12354, 2024.

[10] Xiao, J., Shang, X., Yao, A., and Chua, T.S. Next-QA: Next phase of question-answering to explaining temporal actions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9777-9786, 2021.

[11] Lei, J., Yu, L., Bansal, M., and Berg, T.L. TVQA: Localized, compositional video question answering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1369-1379, 2018.

[12] Xu, J., Mei, T., Yao, T., and Rui, Y. MSR-VTT: A large video description dataset for bridging video and language. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5288-5296, 2016.

[13] Yu, Z., Xu, D., Yu, J., et al. ActivityNet-QA: A dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 9127-9134, 2019.

[14] Poria, S., Cambria, E., Bajpai, R., and Hussain, A. A review of affective computing: From unimodal analysis to multimodal fusion. Information Fusion, 37: 98-125, 2017.

[15] Zadeh, A., Liang, P.P., Poria, S., et al. Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 2236-2246, 2018.

[16] Busso, C., Bulut, M., Lee, C.C., et al. IEMOCAP: Interactive emotional dyadic motion capture database. Language Resources and Evaluation, 42(4): 335-359, 2008.

[17] Wang, H., Wu, Y., Jiang, B., et al. Learning cross-modal embeddings with adversarial triplet loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11354-11363, 2019.

[18] Gemmeke, J.F., Ellis, D.P., Freedman, D., et al. Audio Set: An ontology and human-labeled dataset for audio events. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 776-780, 2017.

[19] Morgado, P., Vasconcelos, N., and Misra, I. Audio-visual instance discrimination with cross-modal agreement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12475-12486, 2021.

[20] Chen, Y., Zhang, Y., Wang, X., et al. Wav2Lip: Accurately lip-syncing videos in the wild. In Proceedings of the ACM International Conference on Multimedia, pages 3527-3535, 2020.

[21] Krishna, R., Hata, K., Ren, F., et al. Dense-captioning events in videos. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 706-715, 2017.

[22] Zhou, L., Zhou, Y., Corso, J.J., Socher, R., and Xiong, C. End-to-end dense video captioning with masked transformer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 8739-8748, 2018.

[23] Carreira, J. and Zisserman, A. Quo vadis, action recognition? A new model and the Kinetics dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6299-6308, 2017.

[24] Kay, W., Carreira, J., Simonyan, K., et al. The Kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017.

[25] Caba Heilbron, F., Escorcia, V., Ghanem, B., and Carlos Niebles, J. ActivityNet: A large-scale video benchmark for human activity understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 961-970, 2015.

[26] He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770-778, 2016.

[27] Feichtenhofer, C., Fan, H., Malik, J., and He, K. SlowFast networks for video recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 6202-6211, 2019.

[28] Bertasius, G., Wang, H., and Torresani, L. Is space-time attention all you need for video understanding? In Proceedings of the International Conference on Machine Learning (ICML), pages 813-824, 2021.

[29] Dosovitskiy, A., Beyer, L., Kolesnikov, A., et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.

[30] Arnab, A., Dehghani, M., Heigold, G., et al. ViViT: A video vision transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 6836-6846, 2021.

[31] Hershey, S., Chaudhuri, S., Ellis, D.P., et al. CNN architectures for large-scale audio classification. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 131-135, 2017.

[32] Kong, Q., Cao, Y., Iqbal, T., et al. PANNs: Large-scale pretrained audio neural networks for audio pattern recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28: 2880-2894, 2020.

[33] Baevski, A., Zhou, H., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), pages 12449-12460, 2020.

[34] Elizalde, B., Deshmukh, S., Al Ismail, M., and Wang, H. CLAP: Learning audio concepts from natural language supervision. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1-5, 2023.

[35] Ramaswamy, J., Das, S., and Jose, J. Hybrid attention network for audio-visual event localization. In Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1346-1355, 2020.

[36] Zhou, J., Zheng, L., Zhong, Y., Hao, S., and Wang, M. MMIL-Transformer: Multimodal multiple instance learning with transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2344-2353, 2021.

[37] Pearl, J. and Mackenzie, D. The Book of Why: The New Science of Cause and Effect. Basic Books, 2018.

[38] Vaswani, A., Shazeer, N., Parmar, N., et al. Attention is all you need. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), pages 5998-6008, 2017.

[39] Chen, Z., Liu, H., Zhang, W., Li, X., and Wang, Y. StreamAV: Real-time streaming audio-visual understanding with causal transformers. In Proceedings of the International Conference on Computer Vision (ICCV), pages 8765-8774, 2023.
